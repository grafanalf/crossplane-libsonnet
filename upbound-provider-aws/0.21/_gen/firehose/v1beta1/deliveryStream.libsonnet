{
  local d = (import 'doc-util/main.libsonnet'),
  '#':: d.pkg(name='deliveryStream', url='', help='"DeliveryStream is the Schema for the DeliveryStreams API. Provides a AWS Kinesis Firehose Delivery Stream"'),
  '#metadata':: d.obj(help='"ObjectMeta is metadata that all persisted resources must have, which includes all objects users must create."'),
  metadata: {
    '#withAnnotations':: d.fn(help='"Annotations is an unstructured key value map stored with a resource that may be set by external tools to store and retrieve arbitrary metadata. They are not queryable and should be preserved when modifying objects. More info: http://kubernetes.io/docs/user-guide/annotations"', args=[d.arg(name='annotations', type=d.T.object)]),
    withAnnotations(annotations): { metadata+: { annotations: annotations } },
    '#withAnnotationsMixin':: d.fn(help='"Annotations is an unstructured key value map stored with a resource that may be set by external tools to store and retrieve arbitrary metadata. They are not queryable and should be preserved when modifying objects. More info: http://kubernetes.io/docs/user-guide/annotations"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='annotations', type=d.T.object)]),
    withAnnotationsMixin(annotations): { metadata+: { annotations+: annotations } },
    '#withClusterName':: d.fn(help='"The name of the cluster which the object belongs to. This is used to distinguish resources with same name and namespace in different clusters. This field is not set anywhere right now and apiserver is going to ignore it if set in create or update request."', args=[d.arg(name='clusterName', type=d.T.string)]),
    withClusterName(clusterName): { metadata+: { clusterName: clusterName } },
    '#withCreationTimestamp':: d.fn(help='"Time is a wrapper around time.Time which supports correct marshaling to YAML and JSON.  Wrappers are provided for many of the factory methods that the time package offers."', args=[d.arg(name='creationTimestamp', type=d.T.string)]),
    withCreationTimestamp(creationTimestamp): { metadata+: { creationTimestamp: creationTimestamp } },
    '#withDeletionGracePeriodSeconds':: d.fn(help='"Number of seconds allowed for this object to gracefully terminate before it will be removed from the system. Only set when deletionTimestamp is also set. May only be shortened. Read-only."', args=[d.arg(name='deletionGracePeriodSeconds', type=d.T.integer)]),
    withDeletionGracePeriodSeconds(deletionGracePeriodSeconds): { metadata+: { deletionGracePeriodSeconds: deletionGracePeriodSeconds } },
    '#withDeletionTimestamp':: d.fn(help='"Time is a wrapper around time.Time which supports correct marshaling to YAML and JSON.  Wrappers are provided for many of the factory methods that the time package offers."', args=[d.arg(name='deletionTimestamp', type=d.T.string)]),
    withDeletionTimestamp(deletionTimestamp): { metadata+: { deletionTimestamp: deletionTimestamp } },
    '#withFinalizers':: d.fn(help='"Must be empty before the object is deleted from the registry. Each entry is an identifier for the responsible component that will remove the entry from the list. If the deletionTimestamp of the object is non-nil, entries in this list can only be removed. Finalizers may be processed and removed in any order.  Order is NOT enforced because it introduces significant risk of stuck finalizers. finalizers is a shared field, any actor with permission can reorder it. If the finalizer list is processed in order, then this can lead to a situation in which the component responsible for the first finalizer in the list is waiting for a signal (field value, external system, or other) produced by a component responsible for a finalizer later in the list, resulting in a deadlock. Without enforced ordering finalizers are free to order amongst themselves and are not vulnerable to ordering changes in the list."', args=[d.arg(name='finalizers', type=d.T.array)]),
    withFinalizers(finalizers): { metadata+: { finalizers: if std.isArray(v=finalizers) then finalizers else [finalizers] } },
    '#withFinalizersMixin':: d.fn(help='"Must be empty before the object is deleted from the registry. Each entry is an identifier for the responsible component that will remove the entry from the list. If the deletionTimestamp of the object is non-nil, entries in this list can only be removed. Finalizers may be processed and removed in any order.  Order is NOT enforced because it introduces significant risk of stuck finalizers. finalizers is a shared field, any actor with permission can reorder it. If the finalizer list is processed in order, then this can lead to a situation in which the component responsible for the first finalizer in the list is waiting for a signal (field value, external system, or other) produced by a component responsible for a finalizer later in the list, resulting in a deadlock. Without enforced ordering finalizers are free to order amongst themselves and are not vulnerable to ordering changes in the list."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='finalizers', type=d.T.array)]),
    withFinalizersMixin(finalizers): { metadata+: { finalizers+: if std.isArray(v=finalizers) then finalizers else [finalizers] } },
    '#withGenerateName':: d.fn(help='"GenerateName is an optional prefix, used by the server, to generate a unique name ONLY IF the Name field has not been provided. If this field is used, the name returned to the client will be different than the name passed. This value will also be combined with a unique suffix. The provided value has the same validation rules as the Name field, and may be truncated by the length of the suffix required to make the value unique on the server.\\n\\nIf this field is specified and the generated name exists, the server will NOT return a 409 - instead, it will either return 201 Created or 500 with Reason ServerTimeout indicating a unique name could not be found in the time allotted, and the client should retry (optionally after the time indicated in the Retry-After header).\\n\\nApplied only if Name is not specified. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency"', args=[d.arg(name='generateName', type=d.T.string)]),
    withGenerateName(generateName): { metadata+: { generateName: generateName } },
    '#withGeneration':: d.fn(help='"A sequence number representing a specific generation of the desired state. Populated by the system. Read-only."', args=[d.arg(name='generation', type=d.T.integer)]),
    withGeneration(generation): { metadata+: { generation: generation } },
    '#withLabels':: d.fn(help='"Map of string keys and values that can be used to organize and categorize (scope and select) objects. May match selectors of replication controllers and services. More info: http://kubernetes.io/docs/user-guide/labels"', args=[d.arg(name='labels', type=d.T.object)]),
    withLabels(labels): { metadata+: { labels: labels } },
    '#withLabelsMixin':: d.fn(help='"Map of string keys and values that can be used to organize and categorize (scope and select) objects. May match selectors of replication controllers and services. More info: http://kubernetes.io/docs/user-guide/labels"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='labels', type=d.T.object)]),
    withLabelsMixin(labels): { metadata+: { labels+: labels } },
    '#withName':: d.fn(help='"Name must be unique within a namespace. Is required when creating resources, although some resources may allow a client to request the generation of an appropriate name automatically. Name is primarily intended for creation idempotence and configuration definition. Cannot be updated. More info: http://kubernetes.io/docs/user-guide/identifiers#names"', args=[d.arg(name='name', type=d.T.string)]),
    withName(name): { metadata+: { name: name } },
    '#withNamespace':: d.fn(help='"Namespace defines the space within which each name must be unique. An empty namespace is equivalent to the \\"default\\" namespace, but \\"default\\" is the canonical representation. Not all objects are required to be scoped to a namespace - the value of this field for those objects will be empty.\\n\\nMust be a DNS_LABEL. Cannot be updated. More info: http://kubernetes.io/docs/user-guide/namespaces"', args=[d.arg(name='namespace', type=d.T.string)]),
    withNamespace(namespace): { metadata+: { namespace: namespace } },
    '#withOwnerReferences':: d.fn(help='"List of objects depended by this object. If ALL objects in the list have been deleted, this object will be garbage collected. If this object is managed by a controller, then an entry in this list will point to this controller, with the controller field set to true. There cannot be more than one managing controller."', args=[d.arg(name='ownerReferences', type=d.T.array)]),
    withOwnerReferences(ownerReferences): { metadata+: { ownerReferences: if std.isArray(v=ownerReferences) then ownerReferences else [ownerReferences] } },
    '#withOwnerReferencesMixin':: d.fn(help='"List of objects depended by this object. If ALL objects in the list have been deleted, this object will be garbage collected. If this object is managed by a controller, then an entry in this list will point to this controller, with the controller field set to true. There cannot be more than one managing controller."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='ownerReferences', type=d.T.array)]),
    withOwnerReferencesMixin(ownerReferences): { metadata+: { ownerReferences+: if std.isArray(v=ownerReferences) then ownerReferences else [ownerReferences] } },
    '#withResourceVersion':: d.fn(help='"An opaque value that represents the internal version of this object that can be used by clients to determine when objects have changed. May be used for optimistic concurrency, change detection, and the watch operation on a resource or set of resources. Clients must treat these values as opaque and passed unmodified back to the server. They may only be valid for a particular resource or set of resources.\\n\\nPopulated by the system. Read-only. Value must be treated as opaque by clients and . More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency"', args=[d.arg(name='resourceVersion', type=d.T.string)]),
    withResourceVersion(resourceVersion): { metadata+: { resourceVersion: resourceVersion } },
    '#withSelfLink':: d.fn(help='"SelfLink is a URL representing this object. Populated by the system. Read-only.\\n\\nDEPRECATED Kubernetes will stop propagating this field in 1.20 release and the field is planned to be removed in 1.21 release."', args=[d.arg(name='selfLink', type=d.T.string)]),
    withSelfLink(selfLink): { metadata+: { selfLink: selfLink } },
    '#withUid':: d.fn(help='"UID is the unique in time and space value for this object. It is typically generated by the server on successful creation of a resource and is not allowed to change on PUT operations.\\n\\nPopulated by the system. Read-only. More info: http://kubernetes.io/docs/user-guide/identifiers#uids"', args=[d.arg(name='uid', type=d.T.string)]),
    withUid(uid): { metadata+: { uid: uid } },
  },
  '#new':: d.fn(help='new returns an instance of DeliveryStream', args=[d.arg(name='name', type=d.T.string)]),
  new(name): {
    apiVersion: 'firehose.aws.upbound.io/v1beta1',
    kind: 'DeliveryStream',
  } + self.metadata.withName(name=name) + self.metadata.withAnnotations(annotations={
    'tanka.dev/namespaced': 'false',
  }),
  '#spec':: d.obj(help='"DeliveryStreamSpec defines the desired state of DeliveryStream"'),
  spec: {
    '#forProvider':: d.obj(help=''),
    forProvider: {
      '#elasticsearchConfiguration':: d.obj(help='"Configuration options if elasticsearch is the destination. More details are given below."'),
      elasticsearchConfiguration: {
        '#cloudwatchLoggingOptions':: d.obj(help='"The CloudWatch Logging Options for the delivery stream. More details are given below"'),
        cloudwatchLoggingOptions: {
          '#withEnabled':: d.fn(help='"Enables or disables the logging. Defaults to false."', args=[d.arg(name='enabled', type=d.T.boolean)]),
          withEnabled(enabled): { enabled: enabled },
          '#withLogGroupName':: d.fn(help='"The CloudWatch group name for logging. This value is required if enabled is true."', args=[d.arg(name='logGroupName', type=d.T.string)]),
          withLogGroupName(logGroupName): { logGroupName: logGroupName },
          '#withLogStreamName':: d.fn(help='"The CloudWatch log stream name for logging. This value is required if enabled is true."', args=[d.arg(name='logStreamName', type=d.T.string)]),
          withLogStreamName(logStreamName): { logStreamName: logStreamName },
        },
        '#processingConfiguration':: d.obj(help='"The data processing configuration.  More details are given below."'),
        processingConfiguration: {
          '#processors':: d.obj(help='"Array of data processors. More details are given below"'),
          processors: {
            '#parameters':: d.obj(help='"Array of processor parameters. More details are given below"'),
            parameters: {
              '#withParameterName':: d.fn(help='"Parameter name. Valid Values: LambdaArn, NumberOfRetries, MetadataExtractionQuery, JsonParsingEngine, RoleArn, BufferSizeInMBs, BufferIntervalInSeconds, SubRecordType, Delimiter. Validation is done against AWS SDK constants; so that values not explicitly listed may also work."', args=[d.arg(name='parameterName', type=d.T.string)]),
              withParameterName(parameterName): { parameterName: parameterName },
              '#withParameterValue':: d.fn(help='"Parameter value. Must be between 1 and 512 length (inclusive). When providing a Lambda ARN, you should specify the resource version as well."', args=[d.arg(name='parameterValue', type=d.T.string)]),
              withParameterValue(parameterValue): { parameterValue: parameterValue },
            },
            '#withParameters':: d.fn(help='"Array of processor parameters. More details are given below"', args=[d.arg(name='parameters', type=d.T.array)]),
            withParameters(parameters): { parameters: if std.isArray(v=parameters) then parameters else [parameters] },
            '#withParametersMixin':: d.fn(help='"Array of processor parameters. More details are given below"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='parameters', type=d.T.array)]),
            withParametersMixin(parameters): { parameters+: if std.isArray(v=parameters) then parameters else [parameters] },
            '#withType':: d.fn(help='"The type of processor. Valid Values: RecordDeAggregation, Lambda, MetadataExtraction, AppendDelimiterToRecord. Validation is done against AWS SDK constants; so that values not explicitly listed may also work."', args=[d.arg(name='type', type=d.T.string)]),
            withType(type): { type: type },
          },
          '#withEnabled':: d.fn(help='"Enables or disables the logging. Defaults to false."', args=[d.arg(name='enabled', type=d.T.boolean)]),
          withEnabled(enabled): { enabled: enabled },
          '#withProcessors':: d.fn(help='"Array of data processors. More details are given below"', args=[d.arg(name='processors', type=d.T.array)]),
          withProcessors(processors): { processors: if std.isArray(v=processors) then processors else [processors] },
          '#withProcessorsMixin':: d.fn(help='"Array of data processors. More details are given below"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='processors', type=d.T.array)]),
          withProcessorsMixin(processors): { processors+: if std.isArray(v=processors) then processors else [processors] },
        },
        '#roleArnRef':: d.obj(help='"Reference to a Role in iam to populate roleArn."'),
        roleArnRef: {
          '#policy':: d.obj(help='"Policies for referencing."'),
          policy: {
            '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
            withResolution(resolution): { roleArnRef+: { policy+: { resolution: resolution } } },
            '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
            withResolve(resolve): { roleArnRef+: { policy+: { resolve: resolve } } },
          },
          '#withName':: d.fn(help='"Name of the referenced object."', args=[d.arg(name='name', type=d.T.string)]),
          withName(name): { roleArnRef+: { name: name } },
        },
        '#roleArnSelector':: d.obj(help='"Selector for a Role in iam to populate roleArn."'),
        roleArnSelector: {
          '#policy':: d.obj(help='"Policies for selection."'),
          policy: {
            '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
            withResolution(resolution): { roleArnSelector+: { policy+: { resolution: resolution } } },
            '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
            withResolve(resolve): { roleArnSelector+: { policy+: { resolve: resolve } } },
          },
          '#withMatchControllerRef':: d.fn(help='"MatchControllerRef ensures an object with the same controller reference as the selecting object is selected."', args=[d.arg(name='matchControllerRef', type=d.T.boolean)]),
          withMatchControllerRef(matchControllerRef): { roleArnSelector+: { matchControllerRef: matchControllerRef } },
          '#withMatchLabels':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."', args=[d.arg(name='matchLabels', type=d.T.object)]),
          withMatchLabels(matchLabels): { roleArnSelector+: { matchLabels: matchLabels } },
          '#withMatchLabelsMixin':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
          withMatchLabelsMixin(matchLabels): { roleArnSelector+: { matchLabels+: matchLabels } },
        },
        '#vpcConfig':: d.obj(help='"The VPC configuration for the delivery stream to connect to Elastic Search associated with the VPC. More details are given below"'),
        vpcConfig: {
          '#roleArnRef':: d.obj(help='"Reference to a Role in iam to populate roleArn."'),
          roleArnRef: {
            '#policy':: d.obj(help='"Policies for referencing."'),
            policy: {
              '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
              withResolution(resolution): { roleArnRef+: { policy+: { resolution: resolution } } },
              '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
              withResolve(resolve): { roleArnRef+: { policy+: { resolve: resolve } } },
            },
            '#withName':: d.fn(help='"Name of the referenced object."', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { roleArnRef+: { name: name } },
          },
          '#roleArnSelector':: d.obj(help='"Selector for a Role in iam to populate roleArn."'),
          roleArnSelector: {
            '#policy':: d.obj(help='"Policies for selection."'),
            policy: {
              '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
              withResolution(resolution): { roleArnSelector+: { policy+: { resolution: resolution } } },
              '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
              withResolve(resolve): { roleArnSelector+: { policy+: { resolve: resolve } } },
            },
            '#withMatchControllerRef':: d.fn(help='"MatchControllerRef ensures an object with the same controller reference as the selecting object is selected."', args=[d.arg(name='matchControllerRef', type=d.T.boolean)]),
            withMatchControllerRef(matchControllerRef): { roleArnSelector+: { matchControllerRef: matchControllerRef } },
            '#withMatchLabels':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."', args=[d.arg(name='matchLabels', type=d.T.object)]),
            withMatchLabels(matchLabels): { roleArnSelector+: { matchLabels: matchLabels } },
            '#withMatchLabelsMixin':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
            withMatchLabelsMixin(matchLabels): { roleArnSelector+: { matchLabels+: matchLabels } },
          },
          '#withRoleArn':: d.fn(help='"The ARN of the AWS credentials."', args=[d.arg(name='roleArn', type=d.T.string)]),
          withRoleArn(roleArn): { roleArn: roleArn },
          '#withSecurityGroupIds':: d.fn(help='"A list of security group IDs to associate with Kinesis Firehose."', args=[d.arg(name='securityGroupIds', type=d.T.array)]),
          withSecurityGroupIds(securityGroupIds): { securityGroupIds: if std.isArray(v=securityGroupIds) then securityGroupIds else [securityGroupIds] },
          '#withSecurityGroupIdsMixin':: d.fn(help='"A list of security group IDs to associate with Kinesis Firehose."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='securityGroupIds', type=d.T.array)]),
          withSecurityGroupIdsMixin(securityGroupIds): { securityGroupIds+: if std.isArray(v=securityGroupIds) then securityGroupIds else [securityGroupIds] },
          '#withSubnetIds':: d.fn(help='"A list of subnet IDs to associate with Kinesis Firehose."', args=[d.arg(name='subnetIds', type=d.T.array)]),
          withSubnetIds(subnetIds): { subnetIds: if std.isArray(v=subnetIds) then subnetIds else [subnetIds] },
          '#withSubnetIdsMixin':: d.fn(help='"A list of subnet IDs to associate with Kinesis Firehose."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='subnetIds', type=d.T.array)]),
          withSubnetIdsMixin(subnetIds): { subnetIds+: if std.isArray(v=subnetIds) then subnetIds else [subnetIds] },
        },
        '#withBufferingInterval':: d.fn(help='"Buffer incoming data for the specified period of time, in seconds between 60 to 900, before delivering it to the destination.  The default value is 300s."', args=[d.arg(name='bufferingInterval', type=d.T.number)]),
        withBufferingInterval(bufferingInterval): { bufferingInterval: bufferingInterval },
        '#withBufferingSize':: d.fn(help='"Buffer incoming data to the specified size, in MBs between 1 to 100, before delivering it to the destination.  The default value is 5MB."', args=[d.arg(name='bufferingSize', type=d.T.number)]),
        withBufferingSize(bufferingSize): { bufferingSize: bufferingSize },
        '#withCloudwatchLoggingOptions':: d.fn(help='"The CloudWatch Logging Options for the delivery stream. More details are given below"', args=[d.arg(name='cloudwatchLoggingOptions', type=d.T.array)]),
        withCloudwatchLoggingOptions(cloudwatchLoggingOptions): { cloudwatchLoggingOptions: if std.isArray(v=cloudwatchLoggingOptions) then cloudwatchLoggingOptions else [cloudwatchLoggingOptions] },
        '#withCloudwatchLoggingOptionsMixin':: d.fn(help='"The CloudWatch Logging Options for the delivery stream. More details are given below"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='cloudwatchLoggingOptions', type=d.T.array)]),
        withCloudwatchLoggingOptionsMixin(cloudwatchLoggingOptions): { cloudwatchLoggingOptions+: if std.isArray(v=cloudwatchLoggingOptions) then cloudwatchLoggingOptions else [cloudwatchLoggingOptions] },
        '#withClusterEndpoint':: d.fn(help='"The endpoint to use when communicating with the cluster. Conflicts with domain_arn."', args=[d.arg(name='clusterEndpoint', type=d.T.string)]),
        withClusterEndpoint(clusterEndpoint): { clusterEndpoint: clusterEndpoint },
        '#withDomainArn':: d.fn(help='"The ARN of the Amazon ES domain.  The IAM role must have permission for DescribeElasticsearchDomain, DescribeElasticsearchDomains, and DescribeElasticsearchDomainConfig after assuming RoleARN.  The pattern needs to be arn:.*. Conflicts with cluster_endpoint."', args=[d.arg(name='domainArn', type=d.T.string)]),
        withDomainArn(domainArn): { domainArn: domainArn },
        '#withIndexName':: d.fn(help='"The Elasticsearch index name."', args=[d.arg(name='indexName', type=d.T.string)]),
        withIndexName(indexName): { indexName: indexName },
        '#withIndexRotationPeriod':: d.fn(help='"The Elasticsearch index rotation period.  Index rotation appends a timestamp to the IndexName to facilitate expiration of old data.  Valid values are NoRotation, OneHour, OneDay, OneWeek, and OneMonth.  The default value is OneDay."', args=[d.arg(name='indexRotationPeriod', type=d.T.string)]),
        withIndexRotationPeriod(indexRotationPeriod): { indexRotationPeriod: indexRotationPeriod },
        '#withProcessingConfiguration':: d.fn(help='"The data processing configuration.  More details are given below."', args=[d.arg(name='processingConfiguration', type=d.T.array)]),
        withProcessingConfiguration(processingConfiguration): { processingConfiguration: if std.isArray(v=processingConfiguration) then processingConfiguration else [processingConfiguration] },
        '#withProcessingConfigurationMixin':: d.fn(help='"The data processing configuration.  More details are given below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='processingConfiguration', type=d.T.array)]),
        withProcessingConfigurationMixin(processingConfiguration): { processingConfiguration+: if std.isArray(v=processingConfiguration) then processingConfiguration else [processingConfiguration] },
        '#withRetryDuration':: d.fn(help='"The length of time during which Firehose retries delivery after a failure, starting from the initial request and including the first attempt. The default value is 3600 seconds (60 minutes). Firehose does not retry if the value of DurationInSeconds is 0 (zero) or if the first delivery attempt takes longer than the current value."', args=[d.arg(name='retryDuration', type=d.T.number)]),
        withRetryDuration(retryDuration): { retryDuration: retryDuration },
        '#withRoleArn':: d.fn(help='"The ARN of the AWS credentials."', args=[d.arg(name='roleArn', type=d.T.string)]),
        withRoleArn(roleArn): { roleArn: roleArn },
        '#withS3BackupMode':: d.fn(help='"The Amazon S3 backup mode.  Valid values are Disabled and Enabled.  Default value is Disabled."', args=[d.arg(name='s3BackupMode', type=d.T.string)]),
        withS3BackupMode(s3BackupMode): { s3BackupMode: s3BackupMode },
        '#withTypeName':: d.fn(help='"The Elasticsearch type name with maximum length of 100 characters."', args=[d.arg(name='typeName', type=d.T.string)]),
        withTypeName(typeName): { typeName: typeName },
        '#withVpcConfig':: d.fn(help='"The VPC configuration for the delivery stream to connect to Elastic Search associated with the VPC. More details are given below"', args=[d.arg(name='vpcConfig', type=d.T.array)]),
        withVpcConfig(vpcConfig): { vpcConfig: if std.isArray(v=vpcConfig) then vpcConfig else [vpcConfig] },
        '#withVpcConfigMixin':: d.fn(help='"The VPC configuration for the delivery stream to connect to Elastic Search associated with the VPC. More details are given below"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='vpcConfig', type=d.T.array)]),
        withVpcConfigMixin(vpcConfig): { vpcConfig+: if std.isArray(v=vpcConfig) then vpcConfig else [vpcConfig] },
      },
      '#extendedS3Configuration':: d.obj(help='"Enhanced configuration options for the s3 destination. More details are given below."'),
      extendedS3Configuration: {
        '#bucketArnRef':: d.obj(help='"Reference to a Bucket in s3 to populate bucketArn."'),
        bucketArnRef: {
          '#policy':: d.obj(help='"Policies for referencing."'),
          policy: {
            '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
            withResolution(resolution): { bucketArnRef+: { policy+: { resolution: resolution } } },
            '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
            withResolve(resolve): { bucketArnRef+: { policy+: { resolve: resolve } } },
          },
          '#withName':: d.fn(help='"Name of the referenced object."', args=[d.arg(name='name', type=d.T.string)]),
          withName(name): { bucketArnRef+: { name: name } },
        },
        '#bucketArnSelector':: d.obj(help='"Selector for a Bucket in s3 to populate bucketArn."'),
        bucketArnSelector: {
          '#policy':: d.obj(help='"Policies for selection."'),
          policy: {
            '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
            withResolution(resolution): { bucketArnSelector+: { policy+: { resolution: resolution } } },
            '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
            withResolve(resolve): { bucketArnSelector+: { policy+: { resolve: resolve } } },
          },
          '#withMatchControllerRef':: d.fn(help='"MatchControllerRef ensures an object with the same controller reference as the selecting object is selected."', args=[d.arg(name='matchControllerRef', type=d.T.boolean)]),
          withMatchControllerRef(matchControllerRef): { bucketArnSelector+: { matchControllerRef: matchControllerRef } },
          '#withMatchLabels':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."', args=[d.arg(name='matchLabels', type=d.T.object)]),
          withMatchLabels(matchLabels): { bucketArnSelector+: { matchLabels: matchLabels } },
          '#withMatchLabelsMixin':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
          withMatchLabelsMixin(matchLabels): { bucketArnSelector+: { matchLabels+: matchLabels } },
        },
        '#cloudwatchLoggingOptions':: d.obj(help='"The CloudWatch Logging Options for the delivery stream. More details are given below"'),
        cloudwatchLoggingOptions: {
          '#withEnabled':: d.fn(help='"Enables or disables the logging. Defaults to false."', args=[d.arg(name='enabled', type=d.T.boolean)]),
          withEnabled(enabled): { enabled: enabled },
          '#withLogGroupName':: d.fn(help='"The CloudWatch group name for logging. This value is required if enabled is true."', args=[d.arg(name='logGroupName', type=d.T.string)]),
          withLogGroupName(logGroupName): { logGroupName: logGroupName },
          '#withLogStreamName':: d.fn(help='"The CloudWatch log stream name for logging. This value is required if enabled is true."', args=[d.arg(name='logStreamName', type=d.T.string)]),
          withLogStreamName(logStreamName): { logStreamName: logStreamName },
        },
        '#dataFormatConversionConfiguration':: d.obj(help='"Nested argument for the serializer, deserializer, and schema for converting data from the JSON format to the Parquet or ORC format before writing it to Amazon S3. More details given below."'),
        dataFormatConversionConfiguration: {
          '#inputFormatConfiguration':: d.obj(help='"Nested argument that specifies the deserializer that you want Kinesis Data Firehose to use to convert the format of your data from JSON. More details below."'),
          inputFormatConfiguration: {
            '#deserializer':: d.obj(help='"Nested argument that specifies which deserializer to use. You can choose either the Apache Hive JSON SerDe or the OpenX JSON SerDe. More details below."'),
            deserializer: {
              '#hiveJsonSerDe':: d.obj(help='"Nested argument that specifies the native Hive / HCatalog JsonSerDe. More details below."'),
              hiveJsonSerDe: {
                '#withTimestampFormats':: d.fn(help="\"A list of how you want Kinesis Data Firehose to parse the date and time stamps that may be present in your input data JSON. To specify these format strings, follow the pattern syntax of JodaTime's DateTimeFormat format strings. For more information, see Class DateTimeFormat. You can also use the special value millis to parse time stamps in epoch milliseconds. If you don't specify a format, Kinesis Data Firehose uses java.sql.Timestamp::valueOf by default.\"", args=[d.arg(name='timestampFormats', type=d.T.array)]),
                withTimestampFormats(timestampFormats): { timestampFormats: if std.isArray(v=timestampFormats) then timestampFormats else [timestampFormats] },
                '#withTimestampFormatsMixin':: d.fn(help="\"A list of how you want Kinesis Data Firehose to parse the date and time stamps that may be present in your input data JSON. To specify these format strings, follow the pattern syntax of JodaTime's DateTimeFormat format strings. For more information, see Class DateTimeFormat. You can also use the special value millis to parse time stamps in epoch milliseconds. If you don't specify a format, Kinesis Data Firehose uses java.sql.Timestamp::valueOf by default.\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='timestampFormats', type=d.T.array)]),
                withTimestampFormatsMixin(timestampFormats): { timestampFormats+: if std.isArray(v=timestampFormats) then timestampFormats else [timestampFormats] },
              },
              '#openXJsonSerDe':: d.obj(help='"Nested argument that specifies the OpenX SerDe. More details below."'),
              openXJsonSerDe: {
                '#withCaseInsensitive':: d.fn(help='"When set to true, which is the default, Kinesis Data Firehose converts JSON keys to lowercase before deserializing them."', args=[d.arg(name='caseInsensitive', type=d.T.boolean)]),
                withCaseInsensitive(caseInsensitive): { caseInsensitive: caseInsensitive },
                '#withColumnToJsonKeyMappings':: d.fn(help="\"A map of column names to JSON keys that aren't identical to the column names. This is useful when the JSON contains keys that are Hive keywords. For example, timestamp is a Hive keyword. If you have a JSON key named timestamp, set this parameter to { ts = \\\"timestamp\\\" } to map this key to a column named ts.\"", args=[d.arg(name='columnToJsonKeyMappings', type=d.T.object)]),
                withColumnToJsonKeyMappings(columnToJsonKeyMappings): { columnToJsonKeyMappings: columnToJsonKeyMappings },
                '#withColumnToJsonKeyMappingsMixin':: d.fn(help="\"A map of column names to JSON keys that aren't identical to the column names. This is useful when the JSON contains keys that are Hive keywords. For example, timestamp is a Hive keyword. If you have a JSON key named timestamp, set this parameter to { ts = \\\"timestamp\\\" } to map this key to a column named ts.\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='columnToJsonKeyMappings', type=d.T.object)]),
                withColumnToJsonKeyMappingsMixin(columnToJsonKeyMappings): { columnToJsonKeyMappings+: columnToJsonKeyMappings },
                '#withConvertDotsInJsonKeysToUnderscores':: d.fn(help='"When set to true, specifies that the names of the keys include dots and that you want Kinesis Data Firehose to replace them with underscores. This is useful because Apache Hive does not allow dots in column names. For example, if the JSON contains a key whose name is \\"a.b\\", you can define the column name to be \\"a_b\\" when using this option. Defaults to false."', args=[d.arg(name='convertDotsInJsonKeysToUnderscores', type=d.T.boolean)]),
                withConvertDotsInJsonKeysToUnderscores(convertDotsInJsonKeysToUnderscores): { convertDotsInJsonKeysToUnderscores: convertDotsInJsonKeysToUnderscores },
              },
              '#withHiveJsonSerDe':: d.fn(help='"Nested argument that specifies the native Hive / HCatalog JsonSerDe. More details below."', args=[d.arg(name='hiveJsonSerDe', type=d.T.array)]),
              withHiveJsonSerDe(hiveJsonSerDe): { hiveJsonSerDe: if std.isArray(v=hiveJsonSerDe) then hiveJsonSerDe else [hiveJsonSerDe] },
              '#withHiveJsonSerDeMixin':: d.fn(help='"Nested argument that specifies the native Hive / HCatalog JsonSerDe. More details below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='hiveJsonSerDe', type=d.T.array)]),
              withHiveJsonSerDeMixin(hiveJsonSerDe): { hiveJsonSerDe+: if std.isArray(v=hiveJsonSerDe) then hiveJsonSerDe else [hiveJsonSerDe] },
              '#withOpenXJsonSerDe':: d.fn(help='"Nested argument that specifies the OpenX SerDe. More details below."', args=[d.arg(name='openXJsonSerDe', type=d.T.array)]),
              withOpenXJsonSerDe(openXJsonSerDe): { openXJsonSerDe: if std.isArray(v=openXJsonSerDe) then openXJsonSerDe else [openXJsonSerDe] },
              '#withOpenXJsonSerDeMixin':: d.fn(help='"Nested argument that specifies the OpenX SerDe. More details below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='openXJsonSerDe', type=d.T.array)]),
              withOpenXJsonSerDeMixin(openXJsonSerDe): { openXJsonSerDe+: if std.isArray(v=openXJsonSerDe) then openXJsonSerDe else [openXJsonSerDe] },
            },
            '#withDeserializer':: d.fn(help='"Nested argument that specifies which deserializer to use. You can choose either the Apache Hive JSON SerDe or the OpenX JSON SerDe. More details below."', args=[d.arg(name='deserializer', type=d.T.array)]),
            withDeserializer(deserializer): { deserializer: if std.isArray(v=deserializer) then deserializer else [deserializer] },
            '#withDeserializerMixin':: d.fn(help='"Nested argument that specifies which deserializer to use. You can choose either the Apache Hive JSON SerDe or the OpenX JSON SerDe. More details below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='deserializer', type=d.T.array)]),
            withDeserializerMixin(deserializer): { deserializer+: if std.isArray(v=deserializer) then deserializer else [deserializer] },
          },
          '#outputFormatConfiguration':: d.obj(help='"Nested argument that specifies the serializer that you want Kinesis Data Firehose to use to convert the format of your data to the Parquet or ORC format. More details below."'),
          outputFormatConfiguration: {
            '#serializer':: d.obj(help='"Nested argument that specifies which serializer to use. You can choose either the ORC SerDe or the Parquet SerDe. More details below."'),
            serializer: {
              '#orcSerDe':: d.obj(help='"Nested argument that specifies converting data to the ORC format before storing it in Amazon S3. For more information, see Apache ORC. More details below."'),
              orcSerDe: {
                '#withBlockSizeBytes':: d.fn(help='"The Hadoop Distributed File System (HDFS) block size. This is useful if you intend to copy the data from Amazon S3 to HDFS before querying. The default is 256 MiB and the minimum is 64 MiB. Kinesis Data Firehose uses this value for padding calculations."', args=[d.arg(name='blockSizeBytes', type=d.T.number)]),
                withBlockSizeBytes(blockSizeBytes): { blockSizeBytes: blockSizeBytes },
                '#withBloomFilterColumns':: d.fn(help='"A list of column names for which you want Kinesis Data Firehose to create bloom filters."', args=[d.arg(name='bloomFilterColumns', type=d.T.array)]),
                withBloomFilterColumns(bloomFilterColumns): { bloomFilterColumns: if std.isArray(v=bloomFilterColumns) then bloomFilterColumns else [bloomFilterColumns] },
                '#withBloomFilterColumnsMixin':: d.fn(help='"A list of column names for which you want Kinesis Data Firehose to create bloom filters."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='bloomFilterColumns', type=d.T.array)]),
                withBloomFilterColumnsMixin(bloomFilterColumns): { bloomFilterColumns+: if std.isArray(v=bloomFilterColumns) then bloomFilterColumns else [bloomFilterColumns] },
                '#withBloomFilterFalsePositiveProbability':: d.fn(help='"The Bloom filter false positive probability (FPP). The lower the FPP, the bigger the Bloom filter. The default value is 0.05, the minimum is 0, and the maximum is 1."', args=[d.arg(name='bloomFilterFalsePositiveProbability', type=d.T.number)]),
                withBloomFilterFalsePositiveProbability(bloomFilterFalsePositiveProbability): { bloomFilterFalsePositiveProbability: bloomFilterFalsePositiveProbability },
                '#withCompression':: d.fn(help='"The compression code to use over data blocks. The possible values are UNCOMPRESSED, SNAPPY, and GZIP, with the default being SNAPPY. Use SNAPPY for higher decompression speed. Use GZIP if the compression ratio is more important than speed."', args=[d.arg(name='compression', type=d.T.string)]),
                withCompression(compression): { compression: compression },
                '#withDictionaryKeyThreshold':: d.fn(help='"A float that represents the fraction of the total number of non-null rows. To turn off dictionary encoding, set this fraction to a number that is less than the number of distinct keys in a dictionary. To always use dictionary encoding, set this threshold to 1."', args=[d.arg(name='dictionaryKeyThreshold', type=d.T.number)]),
                withDictionaryKeyThreshold(dictionaryKeyThreshold): { dictionaryKeyThreshold: dictionaryKeyThreshold },
                '#withEnablePadding':: d.fn(help='"Set this to true to indicate that you want stripes to be padded to the HDFS block boundaries. This is useful if you intend to copy the data from Amazon S3 to HDFS before querying. The default is false."', args=[d.arg(name='enablePadding', type=d.T.boolean)]),
                withEnablePadding(enablePadding): { enablePadding: enablePadding },
                '#withFormatVersion':: d.fn(help='"The version of the file to write. The possible values are V0_11 and V0_12. The default is V0_12."', args=[d.arg(name='formatVersion', type=d.T.string)]),
                withFormatVersion(formatVersion): { formatVersion: formatVersion },
                '#withPaddingTolerance':: d.fn(help='"A float between 0 and 1 that defines the tolerance for block padding as a decimal fraction of stripe size. The default value is 0.05, which means 5 percent of stripe size. For the default values of 64 MiB ORC stripes and 256 MiB HDFS blocks, the default block padding tolerance of 5 percent reserves a maximum of 3.2 MiB for padding within the 256 MiB block. In such a case, if the available size within the block is more than 3.2 MiB, a new, smaller stripe is inserted to fit within that space. This ensures that no stripe crosses block boundaries and causes remote reads within a node-local task. Kinesis Data Firehose ignores this parameter when enable_padding is false."', args=[d.arg(name='paddingTolerance', type=d.T.number)]),
                withPaddingTolerance(paddingTolerance): { paddingTolerance: paddingTolerance },
                '#withRowIndexStride':: d.fn(help='"The number of rows between index entries. The default is 10000 and the minimum is 1000."', args=[d.arg(name='rowIndexStride', type=d.T.number)]),
                withRowIndexStride(rowIndexStride): { rowIndexStride: rowIndexStride },
                '#withStripeSizeBytes':: d.fn(help='"The number of bytes in each stripe. The default is 64 MiB and the minimum is 8 MiB."', args=[d.arg(name='stripeSizeBytes', type=d.T.number)]),
                withStripeSizeBytes(stripeSizeBytes): { stripeSizeBytes: stripeSizeBytes },
              },
              '#parquetSerDe':: d.obj(help='"Nested argument that specifies converting data to the Parquet format before storing it in Amazon S3. For more information, see Apache Parquet. More details below."'),
              parquetSerDe: {
                '#withBlockSizeBytes':: d.fn(help='"The Hadoop Distributed File System (HDFS) block size. This is useful if you intend to copy the data from Amazon S3 to HDFS before querying. The default is 256 MiB and the minimum is 64 MiB. Kinesis Data Firehose uses this value for padding calculations."', args=[d.arg(name='blockSizeBytes', type=d.T.number)]),
                withBlockSizeBytes(blockSizeBytes): { blockSizeBytes: blockSizeBytes },
                '#withCompression':: d.fn(help='"The compression code to use over data blocks. The possible values are UNCOMPRESSED, SNAPPY, and GZIP, with the default being SNAPPY. Use SNAPPY for higher decompression speed. Use GZIP if the compression ratio is more important than speed."', args=[d.arg(name='compression', type=d.T.string)]),
                withCompression(compression): { compression: compression },
                '#withEnableDictionaryCompression':: d.fn(help='"Indicates whether to enable dictionary compression."', args=[d.arg(name='enableDictionaryCompression', type=d.T.boolean)]),
                withEnableDictionaryCompression(enableDictionaryCompression): { enableDictionaryCompression: enableDictionaryCompression },
                '#withMaxPaddingBytes':: d.fn(help='"The maximum amount of padding to apply. This is useful if you intend to copy the data from Amazon S3 to HDFS before querying. The default is 0."', args=[d.arg(name='maxPaddingBytes', type=d.T.number)]),
                withMaxPaddingBytes(maxPaddingBytes): { maxPaddingBytes: maxPaddingBytes },
                '#withPageSizeBytes':: d.fn(help='"The Parquet page size. Column chunks are divided into pages. A page is conceptually an indivisible unit (in terms of compression and encoding). The minimum value is 64 KiB and the default is 1 MiB."', args=[d.arg(name='pageSizeBytes', type=d.T.number)]),
                withPageSizeBytes(pageSizeBytes): { pageSizeBytes: pageSizeBytes },
                '#withWriterVersion':: d.fn(help='"Indicates the version of row format to output. The possible values are V1 and V2. The default is V1."', args=[d.arg(name='writerVersion', type=d.T.string)]),
                withWriterVersion(writerVersion): { writerVersion: writerVersion },
              },
              '#withOrcSerDe':: d.fn(help='"Nested argument that specifies converting data to the ORC format before storing it in Amazon S3. For more information, see Apache ORC. More details below."', args=[d.arg(name='orcSerDe', type=d.T.array)]),
              withOrcSerDe(orcSerDe): { orcSerDe: if std.isArray(v=orcSerDe) then orcSerDe else [orcSerDe] },
              '#withOrcSerDeMixin':: d.fn(help='"Nested argument that specifies converting data to the ORC format before storing it in Amazon S3. For more information, see Apache ORC. More details below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='orcSerDe', type=d.T.array)]),
              withOrcSerDeMixin(orcSerDe): { orcSerDe+: if std.isArray(v=orcSerDe) then orcSerDe else [orcSerDe] },
              '#withParquetSerDe':: d.fn(help='"Nested argument that specifies converting data to the Parquet format before storing it in Amazon S3. For more information, see Apache Parquet. More details below."', args=[d.arg(name='parquetSerDe', type=d.T.array)]),
              withParquetSerDe(parquetSerDe): { parquetSerDe: if std.isArray(v=parquetSerDe) then parquetSerDe else [parquetSerDe] },
              '#withParquetSerDeMixin':: d.fn(help='"Nested argument that specifies converting data to the Parquet format before storing it in Amazon S3. For more information, see Apache Parquet. More details below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='parquetSerDe', type=d.T.array)]),
              withParquetSerDeMixin(parquetSerDe): { parquetSerDe+: if std.isArray(v=parquetSerDe) then parquetSerDe else [parquetSerDe] },
            },
            '#withSerializer':: d.fn(help='"Nested argument that specifies which serializer to use. You can choose either the ORC SerDe or the Parquet SerDe. More details below."', args=[d.arg(name='serializer', type=d.T.array)]),
            withSerializer(serializer): { serializer: if std.isArray(v=serializer) then serializer else [serializer] },
            '#withSerializerMixin':: d.fn(help='"Nested argument that specifies which serializer to use. You can choose either the ORC SerDe or the Parquet SerDe. More details below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='serializer', type=d.T.array)]),
            withSerializerMixin(serializer): { serializer+: if std.isArray(v=serializer) then serializer else [serializer] },
          },
          '#schemaConfiguration':: d.obj(help='"Nested argument that specifies the AWS Glue Data Catalog table that contains the column information. More details below."'),
          schemaConfiguration: {
            '#roleArnRef':: d.obj(help='"Reference to a Role in iam to populate roleArn."'),
            roleArnRef: {
              '#policy':: d.obj(help='"Policies for referencing."'),
              policy: {
                '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
                withResolution(resolution): { roleArnRef+: { policy+: { resolution: resolution } } },
                '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
                withResolve(resolve): { roleArnRef+: { policy+: { resolve: resolve } } },
              },
              '#withName':: d.fn(help='"Name of the referenced object."', args=[d.arg(name='name', type=d.T.string)]),
              withName(name): { roleArnRef+: { name: name } },
            },
            '#roleArnSelector':: d.obj(help='"Selector for a Role in iam to populate roleArn."'),
            roleArnSelector: {
              '#policy':: d.obj(help='"Policies for selection."'),
              policy: {
                '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
                withResolution(resolution): { roleArnSelector+: { policy+: { resolution: resolution } } },
                '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
                withResolve(resolve): { roleArnSelector+: { policy+: { resolve: resolve } } },
              },
              '#withMatchControllerRef':: d.fn(help='"MatchControllerRef ensures an object with the same controller reference as the selecting object is selected."', args=[d.arg(name='matchControllerRef', type=d.T.boolean)]),
              withMatchControllerRef(matchControllerRef): { roleArnSelector+: { matchControllerRef: matchControllerRef } },
              '#withMatchLabels':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."', args=[d.arg(name='matchLabels', type=d.T.object)]),
              withMatchLabels(matchLabels): { roleArnSelector+: { matchLabels: matchLabels } },
              '#withMatchLabelsMixin':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
              withMatchLabelsMixin(matchLabels): { roleArnSelector+: { matchLabels+: matchLabels } },
            },
            '#tableNameRef':: d.obj(help='"Reference to a CatalogTable in glue to populate tableName."'),
            tableNameRef: {
              '#policy':: d.obj(help='"Policies for referencing."'),
              policy: {
                '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
                withResolution(resolution): { tableNameRef+: { policy+: { resolution: resolution } } },
                '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
                withResolve(resolve): { tableNameRef+: { policy+: { resolve: resolve } } },
              },
              '#withName':: d.fn(help='"Name of the referenced object."', args=[d.arg(name='name', type=d.T.string)]),
              withName(name): { tableNameRef+: { name: name } },
            },
            '#tableNameSelector':: d.obj(help='"Selector for a CatalogTable in glue to populate tableName."'),
            tableNameSelector: {
              '#policy':: d.obj(help='"Policies for selection."'),
              policy: {
                '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
                withResolution(resolution): { tableNameSelector+: { policy+: { resolution: resolution } } },
                '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
                withResolve(resolve): { tableNameSelector+: { policy+: { resolve: resolve } } },
              },
              '#withMatchControllerRef':: d.fn(help='"MatchControllerRef ensures an object with the same controller reference as the selecting object is selected."', args=[d.arg(name='matchControllerRef', type=d.T.boolean)]),
              withMatchControllerRef(matchControllerRef): { tableNameSelector+: { matchControllerRef: matchControllerRef } },
              '#withMatchLabels':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."', args=[d.arg(name='matchLabels', type=d.T.object)]),
              withMatchLabels(matchLabels): { tableNameSelector+: { matchLabels: matchLabels } },
              '#withMatchLabelsMixin':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
              withMatchLabelsMixin(matchLabels): { tableNameSelector+: { matchLabels+: matchLabels } },
            },
            '#withCatalogId':: d.fn(help="\"The ID of the AWS Glue Data Catalog. If you don't supply this, the AWS account ID is used by default.\"", args=[d.arg(name='catalogId', type=d.T.string)]),
            withCatalogId(catalogId): { catalogId: catalogId },
            '#withDatabaseName':: d.fn(help='"Specifies the name of the AWS Glue database that contains the schema for the output data."', args=[d.arg(name='databaseName', type=d.T.string)]),
            withDatabaseName(databaseName): { databaseName: databaseName },
            '#withRegion':: d.fn(help="\"If you don't specify an AWS Region, the default is the current region.\"", args=[d.arg(name='region', type=d.T.string)]),
            withRegion(region): { region: region },
            '#withRoleArn':: d.fn(help='"The ARN of the AWS credentials."', args=[d.arg(name='roleArn', type=d.T.string)]),
            withRoleArn(roleArn): { roleArn: roleArn },
            '#withTableName':: d.fn(help='"Specifies the AWS Glue table that contains the column information that constitutes your data schema."', args=[d.arg(name='tableName', type=d.T.string)]),
            withTableName(tableName): { tableName: tableName },
            '#withVersionId':: d.fn(help='"Specifies the table version for the output data schema. Defaults to LATEST."', args=[d.arg(name='versionId', type=d.T.string)]),
            withVersionId(versionId): { versionId: versionId },
          },
          '#withEnabled':: d.fn(help='"Enables or disables the logging. Defaults to false."', args=[d.arg(name='enabled', type=d.T.boolean)]),
          withEnabled(enabled): { enabled: enabled },
          '#withInputFormatConfiguration':: d.fn(help='"Nested argument that specifies the deserializer that you want Kinesis Data Firehose to use to convert the format of your data from JSON. More details below."', args=[d.arg(name='inputFormatConfiguration', type=d.T.array)]),
          withInputFormatConfiguration(inputFormatConfiguration): { inputFormatConfiguration: if std.isArray(v=inputFormatConfiguration) then inputFormatConfiguration else [inputFormatConfiguration] },
          '#withInputFormatConfigurationMixin':: d.fn(help='"Nested argument that specifies the deserializer that you want Kinesis Data Firehose to use to convert the format of your data from JSON. More details below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='inputFormatConfiguration', type=d.T.array)]),
          withInputFormatConfigurationMixin(inputFormatConfiguration): { inputFormatConfiguration+: if std.isArray(v=inputFormatConfiguration) then inputFormatConfiguration else [inputFormatConfiguration] },
          '#withOutputFormatConfiguration':: d.fn(help='"Nested argument that specifies the serializer that you want Kinesis Data Firehose to use to convert the format of your data to the Parquet or ORC format. More details below."', args=[d.arg(name='outputFormatConfiguration', type=d.T.array)]),
          withOutputFormatConfiguration(outputFormatConfiguration): { outputFormatConfiguration: if std.isArray(v=outputFormatConfiguration) then outputFormatConfiguration else [outputFormatConfiguration] },
          '#withOutputFormatConfigurationMixin':: d.fn(help='"Nested argument that specifies the serializer that you want Kinesis Data Firehose to use to convert the format of your data to the Parquet or ORC format. More details below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='outputFormatConfiguration', type=d.T.array)]),
          withOutputFormatConfigurationMixin(outputFormatConfiguration): { outputFormatConfiguration+: if std.isArray(v=outputFormatConfiguration) then outputFormatConfiguration else [outputFormatConfiguration] },
          '#withSchemaConfiguration':: d.fn(help='"Nested argument that specifies the AWS Glue Data Catalog table that contains the column information. More details below."', args=[d.arg(name='schemaConfiguration', type=d.T.array)]),
          withSchemaConfiguration(schemaConfiguration): { schemaConfiguration: if std.isArray(v=schemaConfiguration) then schemaConfiguration else [schemaConfiguration] },
          '#withSchemaConfigurationMixin':: d.fn(help='"Nested argument that specifies the AWS Glue Data Catalog table that contains the column information. More details below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='schemaConfiguration', type=d.T.array)]),
          withSchemaConfigurationMixin(schemaConfiguration): { schemaConfiguration+: if std.isArray(v=schemaConfiguration) then schemaConfiguration else [schemaConfiguration] },
        },
        '#dynamicPartitioningConfiguration':: d.obj(help='"The configuration for dynamic partitioning. See Dynamic Partitioning Configuration below for more details."'),
        dynamicPartitioningConfiguration: {
          '#withEnabled':: d.fn(help='"Enables or disables the logging. Defaults to false."', args=[d.arg(name='enabled', type=d.T.boolean)]),
          withEnabled(enabled): { enabled: enabled },
          '#withRetryDuration':: d.fn(help='"The length of time during which Firehose retries delivery after a failure, starting from the initial request and including the first attempt. The default value is 3600 seconds (60 minutes). Firehose does not retry if the value of DurationInSeconds is 0 (zero) or if the first delivery attempt takes longer than the current value."', args=[d.arg(name='retryDuration', type=d.T.number)]),
          withRetryDuration(retryDuration): { retryDuration: retryDuration },
        },
        '#processingConfiguration':: d.obj(help='"The data processing configuration.  More details are given below."'),
        processingConfiguration: {
          '#processors':: d.obj(help='"Array of data processors. More details are given below"'),
          processors: {
            '#parameters':: d.obj(help='"Array of processor parameters. More details are given below"'),
            parameters: {
              '#withParameterName':: d.fn(help='"Parameter name. Valid Values: LambdaArn, NumberOfRetries, MetadataExtractionQuery, JsonParsingEngine, RoleArn, BufferSizeInMBs, BufferIntervalInSeconds, SubRecordType, Delimiter. Validation is done against AWS SDK constants; so that values not explicitly listed may also work."', args=[d.arg(name='parameterName', type=d.T.string)]),
              withParameterName(parameterName): { parameterName: parameterName },
              '#withParameterValue':: d.fn(help='"Parameter value. Must be between 1 and 512 length (inclusive). When providing a Lambda ARN, you should specify the resource version as well."', args=[d.arg(name='parameterValue', type=d.T.string)]),
              withParameterValue(parameterValue): { parameterValue: parameterValue },
            },
            '#withParameters':: d.fn(help='"Array of processor parameters. More details are given below"', args=[d.arg(name='parameters', type=d.T.array)]),
            withParameters(parameters): { parameters: if std.isArray(v=parameters) then parameters else [parameters] },
            '#withParametersMixin':: d.fn(help='"Array of processor parameters. More details are given below"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='parameters', type=d.T.array)]),
            withParametersMixin(parameters): { parameters+: if std.isArray(v=parameters) then parameters else [parameters] },
            '#withType':: d.fn(help='"The type of processor. Valid Values: RecordDeAggregation, Lambda, MetadataExtraction, AppendDelimiterToRecord. Validation is done against AWS SDK constants; so that values not explicitly listed may also work."', args=[d.arg(name='type', type=d.T.string)]),
            withType(type): { type: type },
          },
          '#withEnabled':: d.fn(help='"Enables or disables the logging. Defaults to false."', args=[d.arg(name='enabled', type=d.T.boolean)]),
          withEnabled(enabled): { enabled: enabled },
          '#withProcessors':: d.fn(help='"Array of data processors. More details are given below"', args=[d.arg(name='processors', type=d.T.array)]),
          withProcessors(processors): { processors: if std.isArray(v=processors) then processors else [processors] },
          '#withProcessorsMixin':: d.fn(help='"Array of data processors. More details are given below"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='processors', type=d.T.array)]),
          withProcessorsMixin(processors): { processors+: if std.isArray(v=processors) then processors else [processors] },
        },
        '#roleArnRef':: d.obj(help='"Reference to a Role in iam to populate roleArn."'),
        roleArnRef: {
          '#policy':: d.obj(help='"Policies for referencing."'),
          policy: {
            '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
            withResolution(resolution): { roleArnRef+: { policy+: { resolution: resolution } } },
            '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
            withResolve(resolve): { roleArnRef+: { policy+: { resolve: resolve } } },
          },
          '#withName':: d.fn(help='"Name of the referenced object."', args=[d.arg(name='name', type=d.T.string)]),
          withName(name): { roleArnRef+: { name: name } },
        },
        '#roleArnSelector':: d.obj(help='"Selector for a Role in iam to populate roleArn."'),
        roleArnSelector: {
          '#policy':: d.obj(help='"Policies for selection."'),
          policy: {
            '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
            withResolution(resolution): { roleArnSelector+: { policy+: { resolution: resolution } } },
            '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
            withResolve(resolve): { roleArnSelector+: { policy+: { resolve: resolve } } },
          },
          '#withMatchControllerRef':: d.fn(help='"MatchControllerRef ensures an object with the same controller reference as the selecting object is selected."', args=[d.arg(name='matchControllerRef', type=d.T.boolean)]),
          withMatchControllerRef(matchControllerRef): { roleArnSelector+: { matchControllerRef: matchControllerRef } },
          '#withMatchLabels':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."', args=[d.arg(name='matchLabels', type=d.T.object)]),
          withMatchLabels(matchLabels): { roleArnSelector+: { matchLabels: matchLabels } },
          '#withMatchLabelsMixin':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
          withMatchLabelsMixin(matchLabels): { roleArnSelector+: { matchLabels+: matchLabels } },
        },
        '#s3BackupConfiguration':: d.obj(help='"The configuration for backup in Amazon S3. Required if s3_backup_mode is Enabled. Supports the same fields as s3_configuration object."'),
        s3BackupConfiguration: {
          '#cloudwatchLoggingOptions':: d.obj(help='"The CloudWatch Logging Options for the delivery stream. More details are given below"'),
          cloudwatchLoggingOptions: {
            '#withEnabled':: d.fn(help='"Enables or disables the logging. Defaults to false."', args=[d.arg(name='enabled', type=d.T.boolean)]),
            withEnabled(enabled): { enabled: enabled },
            '#withLogGroupName':: d.fn(help='"The CloudWatch group name for logging. This value is required if enabled is true."', args=[d.arg(name='logGroupName', type=d.T.string)]),
            withLogGroupName(logGroupName): { logGroupName: logGroupName },
            '#withLogStreamName':: d.fn(help='"The CloudWatch log stream name for logging. This value is required if enabled is true."', args=[d.arg(name='logStreamName', type=d.T.string)]),
            withLogStreamName(logStreamName): { logStreamName: logStreamName },
          },
          '#withBucketArn':: d.fn(help='"The ARN of the S3 bucket"', args=[d.arg(name='bucketArn', type=d.T.string)]),
          withBucketArn(bucketArn): { bucketArn: bucketArn },
          '#withBufferInterval':: d.fn(help='"Buffer incoming data for the specified period of time, in seconds, before delivering it to the destination. The default value is 300."', args=[d.arg(name='bufferInterval', type=d.T.number)]),
          withBufferInterval(bufferInterval): { bufferInterval: bufferInterval },
          '#withBufferSize':: d.fn(help='"Buffer incoming data to the specified size, in MBs, before delivering it to the destination. The default value is 5. We recommend setting SizeInMBs to a value greater than the amount of data you typically ingest into the delivery stream in 10 seconds. For example, if you typically ingest data at 1 MB/sec set SizeInMBs to be 10 MB or higher."', args=[d.arg(name='bufferSize', type=d.T.number)]),
          withBufferSize(bufferSize): { bufferSize: bufferSize },
          '#withCloudwatchLoggingOptions':: d.fn(help='"The CloudWatch Logging Options for the delivery stream. More details are given below"', args=[d.arg(name='cloudwatchLoggingOptions', type=d.T.array)]),
          withCloudwatchLoggingOptions(cloudwatchLoggingOptions): { cloudwatchLoggingOptions: if std.isArray(v=cloudwatchLoggingOptions) then cloudwatchLoggingOptions else [cloudwatchLoggingOptions] },
          '#withCloudwatchLoggingOptionsMixin':: d.fn(help='"The CloudWatch Logging Options for the delivery stream. More details are given below"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='cloudwatchLoggingOptions', type=d.T.array)]),
          withCloudwatchLoggingOptionsMixin(cloudwatchLoggingOptions): { cloudwatchLoggingOptions+: if std.isArray(v=cloudwatchLoggingOptions) then cloudwatchLoggingOptions else [cloudwatchLoggingOptions] },
          '#withCompressionFormat':: d.fn(help='"The compression format. If no value is specified, the default is UNCOMPRESSED. Other supported values are GZIP, ZIP, Snappy, & HADOOP_SNAPPY."', args=[d.arg(name='compressionFormat', type=d.T.string)]),
          withCompressionFormat(compressionFormat): { compressionFormat: compressionFormat },
          '#withErrorOutputPrefix':: d.fn(help='"Prefix added to failed records before writing them to S3. Not currently supported for redshift destination. This prefix appears immediately following the bucket name. For information about how to specify this prefix, see Custom Prefixes for Amazon S3 Objects."', args=[d.arg(name='errorOutputPrefix', type=d.T.string)]),
          withErrorOutputPrefix(errorOutputPrefix): { errorOutputPrefix: errorOutputPrefix },
          '#withKmsKeyArn':: d.fn(help='"Specifies the KMS key ARN the stream will use to encrypt data. If not set, no encryption will be used."', args=[d.arg(name='kmsKeyArn', type=d.T.string)]),
          withKmsKeyArn(kmsKeyArn): { kmsKeyArn: kmsKeyArn },
          '#withPrefix':: d.fn(help='"The \\"YYYY/MM/DD/HH\\" time format prefix is automatically used for delivered S3 files. You can specify an extra prefix to be added in front of the time format prefix. Note that if the prefix ends with a slash, it appears as a folder in the S3 bucket"', args=[d.arg(name='prefix', type=d.T.string)]),
          withPrefix(prefix): { prefix: prefix },
          '#withRoleArn':: d.fn(help='"The ARN of the AWS credentials."', args=[d.arg(name='roleArn', type=d.T.string)]),
          withRoleArn(roleArn): { roleArn: roleArn },
        },
        '#withBucketArn':: d.fn(help='"The ARN of the S3 bucket"', args=[d.arg(name='bucketArn', type=d.T.string)]),
        withBucketArn(bucketArn): { bucketArn: bucketArn },
        '#withBufferInterval':: d.fn(help='"Buffer incoming data for the specified period of time, in seconds, before delivering it to the destination. The default value is 300."', args=[d.arg(name='bufferInterval', type=d.T.number)]),
        withBufferInterval(bufferInterval): { bufferInterval: bufferInterval },
        '#withBufferSize':: d.fn(help='"Buffer incoming data to the specified size, in MBs, before delivering it to the destination. The default value is 5. We recommend setting SizeInMBs to a value greater than the amount of data you typically ingest into the delivery stream in 10 seconds. For example, if you typically ingest data at 1 MB/sec set SizeInMBs to be 10 MB or higher."', args=[d.arg(name='bufferSize', type=d.T.number)]),
        withBufferSize(bufferSize): { bufferSize: bufferSize },
        '#withCloudwatchLoggingOptions':: d.fn(help='"The CloudWatch Logging Options for the delivery stream. More details are given below"', args=[d.arg(name='cloudwatchLoggingOptions', type=d.T.array)]),
        withCloudwatchLoggingOptions(cloudwatchLoggingOptions): { cloudwatchLoggingOptions: if std.isArray(v=cloudwatchLoggingOptions) then cloudwatchLoggingOptions else [cloudwatchLoggingOptions] },
        '#withCloudwatchLoggingOptionsMixin':: d.fn(help='"The CloudWatch Logging Options for the delivery stream. More details are given below"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='cloudwatchLoggingOptions', type=d.T.array)]),
        withCloudwatchLoggingOptionsMixin(cloudwatchLoggingOptions): { cloudwatchLoggingOptions+: if std.isArray(v=cloudwatchLoggingOptions) then cloudwatchLoggingOptions else [cloudwatchLoggingOptions] },
        '#withCompressionFormat':: d.fn(help='"The compression format. If no value is specified, the default is UNCOMPRESSED. Other supported values are GZIP, ZIP, Snappy, & HADOOP_SNAPPY."', args=[d.arg(name='compressionFormat', type=d.T.string)]),
        withCompressionFormat(compressionFormat): { compressionFormat: compressionFormat },
        '#withDataFormatConversionConfiguration':: d.fn(help='"Nested argument for the serializer, deserializer, and schema for converting data from the JSON format to the Parquet or ORC format before writing it to Amazon S3. More details given below."', args=[d.arg(name='dataFormatConversionConfiguration', type=d.T.array)]),
        withDataFormatConversionConfiguration(dataFormatConversionConfiguration): { dataFormatConversionConfiguration: if std.isArray(v=dataFormatConversionConfiguration) then dataFormatConversionConfiguration else [dataFormatConversionConfiguration] },
        '#withDataFormatConversionConfigurationMixin':: d.fn(help='"Nested argument for the serializer, deserializer, and schema for converting data from the JSON format to the Parquet or ORC format before writing it to Amazon S3. More details given below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='dataFormatConversionConfiguration', type=d.T.array)]),
        withDataFormatConversionConfigurationMixin(dataFormatConversionConfiguration): { dataFormatConversionConfiguration+: if std.isArray(v=dataFormatConversionConfiguration) then dataFormatConversionConfiguration else [dataFormatConversionConfiguration] },
        '#withDynamicPartitioningConfiguration':: d.fn(help='"The configuration for dynamic partitioning. See Dynamic Partitioning Configuration below for more details."', args=[d.arg(name='dynamicPartitioningConfiguration', type=d.T.array)]),
        withDynamicPartitioningConfiguration(dynamicPartitioningConfiguration): { dynamicPartitioningConfiguration: if std.isArray(v=dynamicPartitioningConfiguration) then dynamicPartitioningConfiguration else [dynamicPartitioningConfiguration] },
        '#withDynamicPartitioningConfigurationMixin':: d.fn(help='"The configuration for dynamic partitioning. See Dynamic Partitioning Configuration below for more details."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='dynamicPartitioningConfiguration', type=d.T.array)]),
        withDynamicPartitioningConfigurationMixin(dynamicPartitioningConfiguration): { dynamicPartitioningConfiguration+: if std.isArray(v=dynamicPartitioningConfiguration) then dynamicPartitioningConfiguration else [dynamicPartitioningConfiguration] },
        '#withErrorOutputPrefix':: d.fn(help='"Prefix added to failed records before writing them to S3. Not currently supported for redshift destination. This prefix appears immediately following the bucket name. For information about how to specify this prefix, see Custom Prefixes for Amazon S3 Objects."', args=[d.arg(name='errorOutputPrefix', type=d.T.string)]),
        withErrorOutputPrefix(errorOutputPrefix): { errorOutputPrefix: errorOutputPrefix },
        '#withKmsKeyArn':: d.fn(help='"Specifies the KMS key ARN the stream will use to encrypt data. If not set, no encryption will be used."', args=[d.arg(name='kmsKeyArn', type=d.T.string)]),
        withKmsKeyArn(kmsKeyArn): { kmsKeyArn: kmsKeyArn },
        '#withPrefix':: d.fn(help='"The \\"YYYY/MM/DD/HH\\" time format prefix is automatically used for delivered S3 files. You can specify an extra prefix to be added in front of the time format prefix. Note that if the prefix ends with a slash, it appears as a folder in the S3 bucket"', args=[d.arg(name='prefix', type=d.T.string)]),
        withPrefix(prefix): { prefix: prefix },
        '#withProcessingConfiguration':: d.fn(help='"The data processing configuration.  More details are given below."', args=[d.arg(name='processingConfiguration', type=d.T.array)]),
        withProcessingConfiguration(processingConfiguration): { processingConfiguration: if std.isArray(v=processingConfiguration) then processingConfiguration else [processingConfiguration] },
        '#withProcessingConfigurationMixin':: d.fn(help='"The data processing configuration.  More details are given below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='processingConfiguration', type=d.T.array)]),
        withProcessingConfigurationMixin(processingConfiguration): { processingConfiguration+: if std.isArray(v=processingConfiguration) then processingConfiguration else [processingConfiguration] },
        '#withRoleArn':: d.fn(help='"The ARN of the AWS credentials."', args=[d.arg(name='roleArn', type=d.T.string)]),
        withRoleArn(roleArn): { roleArn: roleArn },
        '#withS3BackupConfiguration':: d.fn(help='"The configuration for backup in Amazon S3. Required if s3_backup_mode is Enabled. Supports the same fields as s3_configuration object."', args=[d.arg(name='s3BackupConfiguration', type=d.T.array)]),
        withS3BackupConfiguration(s3BackupConfiguration): { s3BackupConfiguration: if std.isArray(v=s3BackupConfiguration) then s3BackupConfiguration else [s3BackupConfiguration] },
        '#withS3BackupConfigurationMixin':: d.fn(help='"The configuration for backup in Amazon S3. Required if s3_backup_mode is Enabled. Supports the same fields as s3_configuration object."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='s3BackupConfiguration', type=d.T.array)]),
        withS3BackupConfigurationMixin(s3BackupConfiguration): { s3BackupConfiguration+: if std.isArray(v=s3BackupConfiguration) then s3BackupConfiguration else [s3BackupConfiguration] },
        '#withS3BackupMode':: d.fn(help='"The Amazon S3 backup mode.  Valid values are Disabled and Enabled.  Default value is Disabled."', args=[d.arg(name='s3BackupMode', type=d.T.string)]),
        withS3BackupMode(s3BackupMode): { s3BackupMode: s3BackupMode },
      },
      '#httpEndpointConfiguration':: d.obj(help='"Configuration options if http_endpoint is the destination. requires the user to also specify a s3_configuration block.  More details are given below."'),
      httpEndpointConfiguration: {
        '#accessKeySecretRef':: d.obj(help='"The access key required for Kinesis Firehose to authenticate with the HTTP endpoint selected as the destination."'),
        accessKeySecretRef: {
          '#withKey':: d.fn(help='"The key to select."', args=[d.arg(name='key', type=d.T.string)]),
          withKey(key): { accessKeySecretRef+: { key: key } },
          '#withName':: d.fn(help='"Name of the secret."', args=[d.arg(name='name', type=d.T.string)]),
          withName(name): { accessKeySecretRef+: { name: name } },
          '#withNamespace':: d.fn(help='"Namespace of the secret."', args=[d.arg(name='namespace', type=d.T.string)]),
          withNamespace(namespace): { accessKeySecretRef+: { namespace: namespace } },
        },
        '#cloudwatchLoggingOptions':: d.obj(help='"The CloudWatch Logging Options for the delivery stream. More details are given below"'),
        cloudwatchLoggingOptions: {
          '#withEnabled':: d.fn(help='"Enables or disables the logging. Defaults to false."', args=[d.arg(name='enabled', type=d.T.boolean)]),
          withEnabled(enabled): { enabled: enabled },
          '#withLogGroupName':: d.fn(help='"The CloudWatch group name for logging. This value is required if enabled is true."', args=[d.arg(name='logGroupName', type=d.T.string)]),
          withLogGroupName(logGroupName): { logGroupName: logGroupName },
          '#withLogStreamName':: d.fn(help='"The CloudWatch log stream name for logging. This value is required if enabled is true."', args=[d.arg(name='logStreamName', type=d.T.string)]),
          withLogStreamName(logStreamName): { logStreamName: logStreamName },
        },
        '#processingConfiguration':: d.obj(help='"The data processing configuration.  More details are given below."'),
        processingConfiguration: {
          '#processors':: d.obj(help='"Array of data processors. More details are given below"'),
          processors: {
            '#parameters':: d.obj(help='"Array of processor parameters. More details are given below"'),
            parameters: {
              '#withParameterName':: d.fn(help='"Parameter name. Valid Values: LambdaArn, NumberOfRetries, MetadataExtractionQuery, JsonParsingEngine, RoleArn, BufferSizeInMBs, BufferIntervalInSeconds, SubRecordType, Delimiter. Validation is done against AWS SDK constants; so that values not explicitly listed may also work."', args=[d.arg(name='parameterName', type=d.T.string)]),
              withParameterName(parameterName): { parameterName: parameterName },
              '#withParameterValue':: d.fn(help='"Parameter value. Must be between 1 and 512 length (inclusive). When providing a Lambda ARN, you should specify the resource version as well."', args=[d.arg(name='parameterValue', type=d.T.string)]),
              withParameterValue(parameterValue): { parameterValue: parameterValue },
            },
            '#withParameters':: d.fn(help='"Array of processor parameters. More details are given below"', args=[d.arg(name='parameters', type=d.T.array)]),
            withParameters(parameters): { parameters: if std.isArray(v=parameters) then parameters else [parameters] },
            '#withParametersMixin':: d.fn(help='"Array of processor parameters. More details are given below"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='parameters', type=d.T.array)]),
            withParametersMixin(parameters): { parameters+: if std.isArray(v=parameters) then parameters else [parameters] },
            '#withType':: d.fn(help='"The type of processor. Valid Values: RecordDeAggregation, Lambda, MetadataExtraction, AppendDelimiterToRecord. Validation is done against AWS SDK constants; so that values not explicitly listed may also work."', args=[d.arg(name='type', type=d.T.string)]),
            withType(type): { type: type },
          },
          '#withEnabled':: d.fn(help='"Enables or disables the logging. Defaults to false."', args=[d.arg(name='enabled', type=d.T.boolean)]),
          withEnabled(enabled): { enabled: enabled },
          '#withProcessors':: d.fn(help='"Array of data processors. More details are given below"', args=[d.arg(name='processors', type=d.T.array)]),
          withProcessors(processors): { processors: if std.isArray(v=processors) then processors else [processors] },
          '#withProcessorsMixin':: d.fn(help='"Array of data processors. More details are given below"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='processors', type=d.T.array)]),
          withProcessorsMixin(processors): { processors+: if std.isArray(v=processors) then processors else [processors] },
        },
        '#requestConfiguration':: d.obj(help='"The request configuration.  More details are given below."'),
        requestConfiguration: {
          '#commonAttributes':: d.obj(help='"Describes the metadata sent to the HTTP endpoint destination. More details are given below"'),
          commonAttributes: {
            '#withName':: d.fn(help='"The HTTP endpoint name."', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { name: name },
            '#withValue':: d.fn(help='"The value of the HTTP endpoint common attribute."', args=[d.arg(name='value', type=d.T.string)]),
            withValue(value): { value: value },
          },
          '#withCommonAttributes':: d.fn(help='"Describes the metadata sent to the HTTP endpoint destination. More details are given below"', args=[d.arg(name='commonAttributes', type=d.T.array)]),
          withCommonAttributes(commonAttributes): { commonAttributes: if std.isArray(v=commonAttributes) then commonAttributes else [commonAttributes] },
          '#withCommonAttributesMixin':: d.fn(help='"Describes the metadata sent to the HTTP endpoint destination. More details are given below"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='commonAttributes', type=d.T.array)]),
          withCommonAttributesMixin(commonAttributes): { commonAttributes+: if std.isArray(v=commonAttributes) then commonAttributes else [commonAttributes] },
          '#withContentEncoding':: d.fn(help='"Kinesis Data Firehose uses the content encoding to compress the body of a request before sending the request to the destination. Valid values are NONE and GZIP.  Default value is NONE."', args=[d.arg(name='contentEncoding', type=d.T.string)]),
          withContentEncoding(contentEncoding): { contentEncoding: contentEncoding },
        },
        '#roleArnRef':: d.obj(help='"Reference to a Role in iam to populate roleArn."'),
        roleArnRef: {
          '#policy':: d.obj(help='"Policies for referencing."'),
          policy: {
            '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
            withResolution(resolution): { roleArnRef+: { policy+: { resolution: resolution } } },
            '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
            withResolve(resolve): { roleArnRef+: { policy+: { resolve: resolve } } },
          },
          '#withName':: d.fn(help='"Name of the referenced object."', args=[d.arg(name='name', type=d.T.string)]),
          withName(name): { roleArnRef+: { name: name } },
        },
        '#roleArnSelector':: d.obj(help='"Selector for a Role in iam to populate roleArn."'),
        roleArnSelector: {
          '#policy':: d.obj(help='"Policies for selection."'),
          policy: {
            '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
            withResolution(resolution): { roleArnSelector+: { policy+: { resolution: resolution } } },
            '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
            withResolve(resolve): { roleArnSelector+: { policy+: { resolve: resolve } } },
          },
          '#withMatchControllerRef':: d.fn(help='"MatchControllerRef ensures an object with the same controller reference as the selecting object is selected."', args=[d.arg(name='matchControllerRef', type=d.T.boolean)]),
          withMatchControllerRef(matchControllerRef): { roleArnSelector+: { matchControllerRef: matchControllerRef } },
          '#withMatchLabels':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."', args=[d.arg(name='matchLabels', type=d.T.object)]),
          withMatchLabels(matchLabels): { roleArnSelector+: { matchLabels: matchLabels } },
          '#withMatchLabelsMixin':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
          withMatchLabelsMixin(matchLabels): { roleArnSelector+: { matchLabels+: matchLabels } },
        },
        '#withBufferingInterval':: d.fn(help='"Buffer incoming data for the specified period of time, in seconds between 60 to 900, before delivering it to the destination.  The default value is 300s."', args=[d.arg(name='bufferingInterval', type=d.T.number)]),
        withBufferingInterval(bufferingInterval): { bufferingInterval: bufferingInterval },
        '#withBufferingSize':: d.fn(help='"Buffer incoming data to the specified size, in MBs between 1 to 100, before delivering it to the destination.  The default value is 5MB."', args=[d.arg(name='bufferingSize', type=d.T.number)]),
        withBufferingSize(bufferingSize): { bufferingSize: bufferingSize },
        '#withCloudwatchLoggingOptions':: d.fn(help='"The CloudWatch Logging Options for the delivery stream. More details are given below"', args=[d.arg(name='cloudwatchLoggingOptions', type=d.T.array)]),
        withCloudwatchLoggingOptions(cloudwatchLoggingOptions): { cloudwatchLoggingOptions: if std.isArray(v=cloudwatchLoggingOptions) then cloudwatchLoggingOptions else [cloudwatchLoggingOptions] },
        '#withCloudwatchLoggingOptionsMixin':: d.fn(help='"The CloudWatch Logging Options for the delivery stream. More details are given below"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='cloudwatchLoggingOptions', type=d.T.array)]),
        withCloudwatchLoggingOptionsMixin(cloudwatchLoggingOptions): { cloudwatchLoggingOptions+: if std.isArray(v=cloudwatchLoggingOptions) then cloudwatchLoggingOptions else [cloudwatchLoggingOptions] },
        '#withName':: d.fn(help='"The HTTP endpoint name."', args=[d.arg(name='name', type=d.T.string)]),
        withName(name): { name: name },
        '#withProcessingConfiguration':: d.fn(help='"The data processing configuration.  More details are given below."', args=[d.arg(name='processingConfiguration', type=d.T.array)]),
        withProcessingConfiguration(processingConfiguration): { processingConfiguration: if std.isArray(v=processingConfiguration) then processingConfiguration else [processingConfiguration] },
        '#withProcessingConfigurationMixin':: d.fn(help='"The data processing configuration.  More details are given below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='processingConfiguration', type=d.T.array)]),
        withProcessingConfigurationMixin(processingConfiguration): { processingConfiguration+: if std.isArray(v=processingConfiguration) then processingConfiguration else [processingConfiguration] },
        '#withRequestConfiguration':: d.fn(help='"The request configuration.  More details are given below."', args=[d.arg(name='requestConfiguration', type=d.T.array)]),
        withRequestConfiguration(requestConfiguration): { requestConfiguration: if std.isArray(v=requestConfiguration) then requestConfiguration else [requestConfiguration] },
        '#withRequestConfigurationMixin':: d.fn(help='"The request configuration.  More details are given below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='requestConfiguration', type=d.T.array)]),
        withRequestConfigurationMixin(requestConfiguration): { requestConfiguration+: if std.isArray(v=requestConfiguration) then requestConfiguration else [requestConfiguration] },
        '#withRetryDuration':: d.fn(help='"The length of time during which Firehose retries delivery after a failure, starting from the initial request and including the first attempt. The default value is 3600 seconds (60 minutes). Firehose does not retry if the value of DurationInSeconds is 0 (zero) or if the first delivery attempt takes longer than the current value."', args=[d.arg(name='retryDuration', type=d.T.number)]),
        withRetryDuration(retryDuration): { retryDuration: retryDuration },
        '#withRoleArn':: d.fn(help='"The ARN of the AWS credentials."', args=[d.arg(name='roleArn', type=d.T.string)]),
        withRoleArn(roleArn): { roleArn: roleArn },
        '#withS3BackupMode':: d.fn(help='"The Amazon S3 backup mode.  Valid values are Disabled and Enabled.  Default value is Disabled."', args=[d.arg(name='s3BackupMode', type=d.T.string)]),
        withS3BackupMode(s3BackupMode): { s3BackupMode: s3BackupMode },
        '#withUrl':: d.fn(help='"The HTTP endpoint URL to which Kinesis Firehose sends your data."', args=[d.arg(name='url', type=d.T.string)]),
        withUrl(url): { url: url },
      },
      '#kinesisSourceConfiguration':: d.obj(help='"Allows the ability to specify the kinesis stream that is used as the source of the firehose delivery stream."'),
      kinesisSourceConfiguration: {
        '#withKinesisStreamArn':: d.fn(help='"The kinesis stream used as the source of the firehose delivery stream."', args=[d.arg(name='kinesisStreamArn', type=d.T.string)]),
        withKinesisStreamArn(kinesisStreamArn): { kinesisStreamArn: kinesisStreamArn },
        '#withRoleArn':: d.fn(help='"The ARN of the AWS credentials."', args=[d.arg(name='roleArn', type=d.T.string)]),
        withRoleArn(roleArn): { roleArn: roleArn },
      },
      '#redshiftConfiguration':: d.obj(help='"Configuration options if redshift is the destination. Using redshift_configuration requires the user to also specify a s3_configuration block. More details are given below."'),
      redshiftConfiguration: {
        '#cloudwatchLoggingOptions':: d.obj(help='"The CloudWatch Logging Options for the delivery stream. More details are given below"'),
        cloudwatchLoggingOptions: {
          '#withEnabled':: d.fn(help='"Enables or disables the logging. Defaults to false."', args=[d.arg(name='enabled', type=d.T.boolean)]),
          withEnabled(enabled): { enabled: enabled },
          '#withLogGroupName':: d.fn(help='"The CloudWatch group name for logging. This value is required if enabled is true."', args=[d.arg(name='logGroupName', type=d.T.string)]),
          withLogGroupName(logGroupName): { logGroupName: logGroupName },
          '#withLogStreamName':: d.fn(help='"The CloudWatch log stream name for logging. This value is required if enabled is true."', args=[d.arg(name='logStreamName', type=d.T.string)]),
          withLogStreamName(logStreamName): { logStreamName: logStreamName },
        },
        '#passwordSecretRef':: d.obj(help='"The password for the username above."'),
        passwordSecretRef: {
          '#withKey':: d.fn(help='"The key to select."', args=[d.arg(name='key', type=d.T.string)]),
          withKey(key): { passwordSecretRef+: { key: key } },
          '#withName':: d.fn(help='"Name of the secret."', args=[d.arg(name='name', type=d.T.string)]),
          withName(name): { passwordSecretRef+: { name: name } },
          '#withNamespace':: d.fn(help='"Namespace of the secret."', args=[d.arg(name='namespace', type=d.T.string)]),
          withNamespace(namespace): { passwordSecretRef+: { namespace: namespace } },
        },
        '#processingConfiguration':: d.obj(help='"The data processing configuration.  More details are given below."'),
        processingConfiguration: {
          '#processors':: d.obj(help='"Array of data processors. More details are given below"'),
          processors: {
            '#parameters':: d.obj(help='"Array of processor parameters. More details are given below"'),
            parameters: {
              '#withParameterName':: d.fn(help='"Parameter name. Valid Values: LambdaArn, NumberOfRetries, MetadataExtractionQuery, JsonParsingEngine, RoleArn, BufferSizeInMBs, BufferIntervalInSeconds, SubRecordType, Delimiter. Validation is done against AWS SDK constants; so that values not explicitly listed may also work."', args=[d.arg(name='parameterName', type=d.T.string)]),
              withParameterName(parameterName): { parameterName: parameterName },
              '#withParameterValue':: d.fn(help='"Parameter value. Must be between 1 and 512 length (inclusive). When providing a Lambda ARN, you should specify the resource version as well."', args=[d.arg(name='parameterValue', type=d.T.string)]),
              withParameterValue(parameterValue): { parameterValue: parameterValue },
            },
            '#withParameters':: d.fn(help='"Array of processor parameters. More details are given below"', args=[d.arg(name='parameters', type=d.T.array)]),
            withParameters(parameters): { parameters: if std.isArray(v=parameters) then parameters else [parameters] },
            '#withParametersMixin':: d.fn(help='"Array of processor parameters. More details are given below"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='parameters', type=d.T.array)]),
            withParametersMixin(parameters): { parameters+: if std.isArray(v=parameters) then parameters else [parameters] },
            '#withType':: d.fn(help='"The type of processor. Valid Values: RecordDeAggregation, Lambda, MetadataExtraction, AppendDelimiterToRecord. Validation is done against AWS SDK constants; so that values not explicitly listed may also work."', args=[d.arg(name='type', type=d.T.string)]),
            withType(type): { type: type },
          },
          '#withEnabled':: d.fn(help='"Enables or disables the logging. Defaults to false."', args=[d.arg(name='enabled', type=d.T.boolean)]),
          withEnabled(enabled): { enabled: enabled },
          '#withProcessors':: d.fn(help='"Array of data processors. More details are given below"', args=[d.arg(name='processors', type=d.T.array)]),
          withProcessors(processors): { processors: if std.isArray(v=processors) then processors else [processors] },
          '#withProcessorsMixin':: d.fn(help='"Array of data processors. More details are given below"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='processors', type=d.T.array)]),
          withProcessorsMixin(processors): { processors+: if std.isArray(v=processors) then processors else [processors] },
        },
        '#roleArnRef':: d.obj(help='"Reference to a Role in iam to populate roleArn."'),
        roleArnRef: {
          '#policy':: d.obj(help='"Policies for referencing."'),
          policy: {
            '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
            withResolution(resolution): { roleArnRef+: { policy+: { resolution: resolution } } },
            '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
            withResolve(resolve): { roleArnRef+: { policy+: { resolve: resolve } } },
          },
          '#withName':: d.fn(help='"Name of the referenced object."', args=[d.arg(name='name', type=d.T.string)]),
          withName(name): { roleArnRef+: { name: name } },
        },
        '#roleArnSelector':: d.obj(help='"Selector for a Role in iam to populate roleArn."'),
        roleArnSelector: {
          '#policy':: d.obj(help='"Policies for selection."'),
          policy: {
            '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
            withResolution(resolution): { roleArnSelector+: { policy+: { resolution: resolution } } },
            '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
            withResolve(resolve): { roleArnSelector+: { policy+: { resolve: resolve } } },
          },
          '#withMatchControllerRef':: d.fn(help='"MatchControllerRef ensures an object with the same controller reference as the selecting object is selected."', args=[d.arg(name='matchControllerRef', type=d.T.boolean)]),
          withMatchControllerRef(matchControllerRef): { roleArnSelector+: { matchControllerRef: matchControllerRef } },
          '#withMatchLabels':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."', args=[d.arg(name='matchLabels', type=d.T.object)]),
          withMatchLabels(matchLabels): { roleArnSelector+: { matchLabels: matchLabels } },
          '#withMatchLabelsMixin':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
          withMatchLabelsMixin(matchLabels): { roleArnSelector+: { matchLabels+: matchLabels } },
        },
        '#s3BackupConfiguration':: d.obj(help='"The configuration for backup in Amazon S3. Required if s3_backup_mode is Enabled. Supports the same fields as s3_configuration object."'),
        s3BackupConfiguration: {
          '#bucketArnRef':: d.obj(help='"Reference to a Bucket in s3 to populate bucketArn."'),
          bucketArnRef: {
            '#policy':: d.obj(help='"Policies for referencing."'),
            policy: {
              '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
              withResolution(resolution): { bucketArnRef+: { policy+: { resolution: resolution } } },
              '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
              withResolve(resolve): { bucketArnRef+: { policy+: { resolve: resolve } } },
            },
            '#withName':: d.fn(help='"Name of the referenced object."', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { bucketArnRef+: { name: name } },
          },
          '#bucketArnSelector':: d.obj(help='"Selector for a Bucket in s3 to populate bucketArn."'),
          bucketArnSelector: {
            '#policy':: d.obj(help='"Policies for selection."'),
            policy: {
              '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
              withResolution(resolution): { bucketArnSelector+: { policy+: { resolution: resolution } } },
              '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
              withResolve(resolve): { bucketArnSelector+: { policy+: { resolve: resolve } } },
            },
            '#withMatchControllerRef':: d.fn(help='"MatchControllerRef ensures an object with the same controller reference as the selecting object is selected."', args=[d.arg(name='matchControllerRef', type=d.T.boolean)]),
            withMatchControllerRef(matchControllerRef): { bucketArnSelector+: { matchControllerRef: matchControllerRef } },
            '#withMatchLabels':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."', args=[d.arg(name='matchLabels', type=d.T.object)]),
            withMatchLabels(matchLabels): { bucketArnSelector+: { matchLabels: matchLabels } },
            '#withMatchLabelsMixin':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
            withMatchLabelsMixin(matchLabels): { bucketArnSelector+: { matchLabels+: matchLabels } },
          },
          '#cloudwatchLoggingOptions':: d.obj(help='"The CloudWatch Logging Options for the delivery stream. More details are given below"'),
          cloudwatchLoggingOptions: {
            '#withEnabled':: d.fn(help='"Enables or disables the logging. Defaults to false."', args=[d.arg(name='enabled', type=d.T.boolean)]),
            withEnabled(enabled): { enabled: enabled },
            '#withLogGroupName':: d.fn(help='"The CloudWatch group name for logging. This value is required if enabled is true."', args=[d.arg(name='logGroupName', type=d.T.string)]),
            withLogGroupName(logGroupName): { logGroupName: logGroupName },
            '#withLogStreamName':: d.fn(help='"The CloudWatch log stream name for logging. This value is required if enabled is true."', args=[d.arg(name='logStreamName', type=d.T.string)]),
            withLogStreamName(logStreamName): { logStreamName: logStreamName },
          },
          '#roleArnRef':: d.obj(help='"Reference to a Role in iam to populate roleArn."'),
          roleArnRef: {
            '#policy':: d.obj(help='"Policies for referencing."'),
            policy: {
              '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
              withResolution(resolution): { roleArnRef+: { policy+: { resolution: resolution } } },
              '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
              withResolve(resolve): { roleArnRef+: { policy+: { resolve: resolve } } },
            },
            '#withName':: d.fn(help='"Name of the referenced object."', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { roleArnRef+: { name: name } },
          },
          '#roleArnSelector':: d.obj(help='"Selector for a Role in iam to populate roleArn."'),
          roleArnSelector: {
            '#policy':: d.obj(help='"Policies for selection."'),
            policy: {
              '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
              withResolution(resolution): { roleArnSelector+: { policy+: { resolution: resolution } } },
              '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
              withResolve(resolve): { roleArnSelector+: { policy+: { resolve: resolve } } },
            },
            '#withMatchControllerRef':: d.fn(help='"MatchControllerRef ensures an object with the same controller reference as the selecting object is selected."', args=[d.arg(name='matchControllerRef', type=d.T.boolean)]),
            withMatchControllerRef(matchControllerRef): { roleArnSelector+: { matchControllerRef: matchControllerRef } },
            '#withMatchLabels':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."', args=[d.arg(name='matchLabels', type=d.T.object)]),
            withMatchLabels(matchLabels): { roleArnSelector+: { matchLabels: matchLabels } },
            '#withMatchLabelsMixin':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
            withMatchLabelsMixin(matchLabels): { roleArnSelector+: { matchLabels+: matchLabels } },
          },
          '#withBucketArn':: d.fn(help='"The ARN of the S3 bucket"', args=[d.arg(name='bucketArn', type=d.T.string)]),
          withBucketArn(bucketArn): { bucketArn: bucketArn },
          '#withBufferInterval':: d.fn(help='"Buffer incoming data for the specified period of time, in seconds, before delivering it to the destination. The default value is 300."', args=[d.arg(name='bufferInterval', type=d.T.number)]),
          withBufferInterval(bufferInterval): { bufferInterval: bufferInterval },
          '#withBufferSize':: d.fn(help='"Buffer incoming data to the specified size, in MBs, before delivering it to the destination. The default value is 5. We recommend setting SizeInMBs to a value greater than the amount of data you typically ingest into the delivery stream in 10 seconds. For example, if you typically ingest data at 1 MB/sec set SizeInMBs to be 10 MB or higher."', args=[d.arg(name='bufferSize', type=d.T.number)]),
          withBufferSize(bufferSize): { bufferSize: bufferSize },
          '#withCloudwatchLoggingOptions':: d.fn(help='"The CloudWatch Logging Options for the delivery stream. More details are given below"', args=[d.arg(name='cloudwatchLoggingOptions', type=d.T.array)]),
          withCloudwatchLoggingOptions(cloudwatchLoggingOptions): { cloudwatchLoggingOptions: if std.isArray(v=cloudwatchLoggingOptions) then cloudwatchLoggingOptions else [cloudwatchLoggingOptions] },
          '#withCloudwatchLoggingOptionsMixin':: d.fn(help='"The CloudWatch Logging Options for the delivery stream. More details are given below"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='cloudwatchLoggingOptions', type=d.T.array)]),
          withCloudwatchLoggingOptionsMixin(cloudwatchLoggingOptions): { cloudwatchLoggingOptions+: if std.isArray(v=cloudwatchLoggingOptions) then cloudwatchLoggingOptions else [cloudwatchLoggingOptions] },
          '#withCompressionFormat':: d.fn(help='"The compression format. If no value is specified, the default is UNCOMPRESSED. Other supported values are GZIP, ZIP, Snappy, & HADOOP_SNAPPY."', args=[d.arg(name='compressionFormat', type=d.T.string)]),
          withCompressionFormat(compressionFormat): { compressionFormat: compressionFormat },
          '#withErrorOutputPrefix':: d.fn(help='"Prefix added to failed records before writing them to S3. Not currently supported for redshift destination. This prefix appears immediately following the bucket name. For information about how to specify this prefix, see Custom Prefixes for Amazon S3 Objects."', args=[d.arg(name='errorOutputPrefix', type=d.T.string)]),
          withErrorOutputPrefix(errorOutputPrefix): { errorOutputPrefix: errorOutputPrefix },
          '#withKmsKeyArn':: d.fn(help='"Specifies the KMS key ARN the stream will use to encrypt data. If not set, no encryption will be used."', args=[d.arg(name='kmsKeyArn', type=d.T.string)]),
          withKmsKeyArn(kmsKeyArn): { kmsKeyArn: kmsKeyArn },
          '#withPrefix':: d.fn(help='"The \\"YYYY/MM/DD/HH\\" time format prefix is automatically used for delivered S3 files. You can specify an extra prefix to be added in front of the time format prefix. Note that if the prefix ends with a slash, it appears as a folder in the S3 bucket"', args=[d.arg(name='prefix', type=d.T.string)]),
          withPrefix(prefix): { prefix: prefix },
          '#withRoleArn':: d.fn(help='"The ARN of the AWS credentials."', args=[d.arg(name='roleArn', type=d.T.string)]),
          withRoleArn(roleArn): { roleArn: roleArn },
        },
        '#withCloudwatchLoggingOptions':: d.fn(help='"The CloudWatch Logging Options for the delivery stream. More details are given below"', args=[d.arg(name='cloudwatchLoggingOptions', type=d.T.array)]),
        withCloudwatchLoggingOptions(cloudwatchLoggingOptions): { cloudwatchLoggingOptions: if std.isArray(v=cloudwatchLoggingOptions) then cloudwatchLoggingOptions else [cloudwatchLoggingOptions] },
        '#withCloudwatchLoggingOptionsMixin':: d.fn(help='"The CloudWatch Logging Options for the delivery stream. More details are given below"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='cloudwatchLoggingOptions', type=d.T.array)]),
        withCloudwatchLoggingOptionsMixin(cloudwatchLoggingOptions): { cloudwatchLoggingOptions+: if std.isArray(v=cloudwatchLoggingOptions) then cloudwatchLoggingOptions else [cloudwatchLoggingOptions] },
        '#withClusterJdbcurl':: d.fn(help='"The jdbcurl of the redshift cluster."', args=[d.arg(name='clusterJdbcurl', type=d.T.string)]),
        withClusterJdbcurl(clusterJdbcurl): { clusterJdbcurl: clusterJdbcurl },
        '#withCopyOptions':: d.fn(help='"Copy options for copying the data from the s3 intermediate bucket into redshift, for example to change the default delimiter. For valid values, see the AWS documentation"', args=[d.arg(name='copyOptions', type=d.T.string)]),
        withCopyOptions(copyOptions): { copyOptions: copyOptions },
        '#withDataTableColumns':: d.fn(help='"The data table columns that will be targeted by the copy command."', args=[d.arg(name='dataTableColumns', type=d.T.string)]),
        withDataTableColumns(dataTableColumns): { dataTableColumns: dataTableColumns },
        '#withDataTableName':: d.fn(help='"The name of the table in the redshift cluster that the s3 bucket will copy to."', args=[d.arg(name='dataTableName', type=d.T.string)]),
        withDataTableName(dataTableName): { dataTableName: dataTableName },
        '#withProcessingConfiguration':: d.fn(help='"The data processing configuration.  More details are given below."', args=[d.arg(name='processingConfiguration', type=d.T.array)]),
        withProcessingConfiguration(processingConfiguration): { processingConfiguration: if std.isArray(v=processingConfiguration) then processingConfiguration else [processingConfiguration] },
        '#withProcessingConfigurationMixin':: d.fn(help='"The data processing configuration.  More details are given below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='processingConfiguration', type=d.T.array)]),
        withProcessingConfigurationMixin(processingConfiguration): { processingConfiguration+: if std.isArray(v=processingConfiguration) then processingConfiguration else [processingConfiguration] },
        '#withRetryDuration':: d.fn(help='"The length of time during which Firehose retries delivery after a failure, starting from the initial request and including the first attempt. The default value is 3600 seconds (60 minutes). Firehose does not retry if the value of DurationInSeconds is 0 (zero) or if the first delivery attempt takes longer than the current value."', args=[d.arg(name='retryDuration', type=d.T.number)]),
        withRetryDuration(retryDuration): { retryDuration: retryDuration },
        '#withRoleArn':: d.fn(help='"The ARN of the AWS credentials."', args=[d.arg(name='roleArn', type=d.T.string)]),
        withRoleArn(roleArn): { roleArn: roleArn },
        '#withS3BackupConfiguration':: d.fn(help='"The configuration for backup in Amazon S3. Required if s3_backup_mode is Enabled. Supports the same fields as s3_configuration object."', args=[d.arg(name='s3BackupConfiguration', type=d.T.array)]),
        withS3BackupConfiguration(s3BackupConfiguration): { s3BackupConfiguration: if std.isArray(v=s3BackupConfiguration) then s3BackupConfiguration else [s3BackupConfiguration] },
        '#withS3BackupConfigurationMixin':: d.fn(help='"The configuration for backup in Amazon S3. Required if s3_backup_mode is Enabled. Supports the same fields as s3_configuration object."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='s3BackupConfiguration', type=d.T.array)]),
        withS3BackupConfigurationMixin(s3BackupConfiguration): { s3BackupConfiguration+: if std.isArray(v=s3BackupConfiguration) then s3BackupConfiguration else [s3BackupConfiguration] },
        '#withS3BackupMode':: d.fn(help='"The Amazon S3 backup mode.  Valid values are Disabled and Enabled.  Default value is Disabled."', args=[d.arg(name='s3BackupMode', type=d.T.string)]),
        withS3BackupMode(s3BackupMode): { s3BackupMode: s3BackupMode },
        '#withUsername':: d.fn(help='"The username that the firehose delivery stream will assume. It is strongly recommended that the username and password provided is used exclusively for Amazon Kinesis Firehose purposes, and that the permissions for the account are restricted for Amazon Redshift INSERT permissions."', args=[d.arg(name='username', type=d.T.string)]),
        withUsername(username): { username: username },
      },
      '#s3Configuration':: d.obj(help='"Required for non-S3 destinations. For S3 destination, use extended_s3_configuration instead. Configuration options for the s3 destination (or the intermediate bucket if the destination is redshift). More details are given below."'),
      s3Configuration: {
        '#bucketArnRef':: d.obj(help='"Reference to a Bucket in s3 to populate bucketArn."'),
        bucketArnRef: {
          '#policy':: d.obj(help='"Policies for referencing."'),
          policy: {
            '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
            withResolution(resolution): { bucketArnRef+: { policy+: { resolution: resolution } } },
            '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
            withResolve(resolve): { bucketArnRef+: { policy+: { resolve: resolve } } },
          },
          '#withName':: d.fn(help='"Name of the referenced object."', args=[d.arg(name='name', type=d.T.string)]),
          withName(name): { bucketArnRef+: { name: name } },
        },
        '#bucketArnSelector':: d.obj(help='"Selector for a Bucket in s3 to populate bucketArn."'),
        bucketArnSelector: {
          '#policy':: d.obj(help='"Policies for selection."'),
          policy: {
            '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
            withResolution(resolution): { bucketArnSelector+: { policy+: { resolution: resolution } } },
            '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
            withResolve(resolve): { bucketArnSelector+: { policy+: { resolve: resolve } } },
          },
          '#withMatchControllerRef':: d.fn(help='"MatchControllerRef ensures an object with the same controller reference as the selecting object is selected."', args=[d.arg(name='matchControllerRef', type=d.T.boolean)]),
          withMatchControllerRef(matchControllerRef): { bucketArnSelector+: { matchControllerRef: matchControllerRef } },
          '#withMatchLabels':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."', args=[d.arg(name='matchLabels', type=d.T.object)]),
          withMatchLabels(matchLabels): { bucketArnSelector+: { matchLabels: matchLabels } },
          '#withMatchLabelsMixin':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
          withMatchLabelsMixin(matchLabels): { bucketArnSelector+: { matchLabels+: matchLabels } },
        },
        '#cloudwatchLoggingOptions':: d.obj(help='"The CloudWatch Logging Options for the delivery stream. More details are given below"'),
        cloudwatchLoggingOptions: {
          '#withEnabled':: d.fn(help='"Enables or disables the logging. Defaults to false."', args=[d.arg(name='enabled', type=d.T.boolean)]),
          withEnabled(enabled): { enabled: enabled },
          '#withLogGroupName':: d.fn(help='"The CloudWatch group name for logging. This value is required if enabled is true."', args=[d.arg(name='logGroupName', type=d.T.string)]),
          withLogGroupName(logGroupName): { logGroupName: logGroupName },
          '#withLogStreamName':: d.fn(help='"The CloudWatch log stream name for logging. This value is required if enabled is true."', args=[d.arg(name='logStreamName', type=d.T.string)]),
          withLogStreamName(logStreamName): { logStreamName: logStreamName },
        },
        '#roleArnRef':: d.obj(help='"Reference to a Role in iam to populate roleArn."'),
        roleArnRef: {
          '#policy':: d.obj(help='"Policies for referencing."'),
          policy: {
            '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
            withResolution(resolution): { roleArnRef+: { policy+: { resolution: resolution } } },
            '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
            withResolve(resolve): { roleArnRef+: { policy+: { resolve: resolve } } },
          },
          '#withName':: d.fn(help='"Name of the referenced object."', args=[d.arg(name='name', type=d.T.string)]),
          withName(name): { roleArnRef+: { name: name } },
        },
        '#roleArnSelector':: d.obj(help='"Selector for a Role in iam to populate roleArn."'),
        roleArnSelector: {
          '#policy':: d.obj(help='"Policies for selection."'),
          policy: {
            '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
            withResolution(resolution): { roleArnSelector+: { policy+: { resolution: resolution } } },
            '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
            withResolve(resolve): { roleArnSelector+: { policy+: { resolve: resolve } } },
          },
          '#withMatchControllerRef':: d.fn(help='"MatchControllerRef ensures an object with the same controller reference as the selecting object is selected."', args=[d.arg(name='matchControllerRef', type=d.T.boolean)]),
          withMatchControllerRef(matchControllerRef): { roleArnSelector+: { matchControllerRef: matchControllerRef } },
          '#withMatchLabels':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."', args=[d.arg(name='matchLabels', type=d.T.object)]),
          withMatchLabels(matchLabels): { roleArnSelector+: { matchLabels: matchLabels } },
          '#withMatchLabelsMixin':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
          withMatchLabelsMixin(matchLabels): { roleArnSelector+: { matchLabels+: matchLabels } },
        },
        '#withBucketArn':: d.fn(help='"The ARN of the S3 bucket"', args=[d.arg(name='bucketArn', type=d.T.string)]),
        withBucketArn(bucketArn): { bucketArn: bucketArn },
        '#withBufferInterval':: d.fn(help='"Buffer incoming data for the specified period of time, in seconds, before delivering it to the destination. The default value is 300."', args=[d.arg(name='bufferInterval', type=d.T.number)]),
        withBufferInterval(bufferInterval): { bufferInterval: bufferInterval },
        '#withBufferSize':: d.fn(help='"Buffer incoming data to the specified size, in MBs, before delivering it to the destination. The default value is 5. We recommend setting SizeInMBs to a value greater than the amount of data you typically ingest into the delivery stream in 10 seconds. For example, if you typically ingest data at 1 MB/sec set SizeInMBs to be 10 MB or higher."', args=[d.arg(name='bufferSize', type=d.T.number)]),
        withBufferSize(bufferSize): { bufferSize: bufferSize },
        '#withCloudwatchLoggingOptions':: d.fn(help='"The CloudWatch Logging Options for the delivery stream. More details are given below"', args=[d.arg(name='cloudwatchLoggingOptions', type=d.T.array)]),
        withCloudwatchLoggingOptions(cloudwatchLoggingOptions): { cloudwatchLoggingOptions: if std.isArray(v=cloudwatchLoggingOptions) then cloudwatchLoggingOptions else [cloudwatchLoggingOptions] },
        '#withCloudwatchLoggingOptionsMixin':: d.fn(help='"The CloudWatch Logging Options for the delivery stream. More details are given below"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='cloudwatchLoggingOptions', type=d.T.array)]),
        withCloudwatchLoggingOptionsMixin(cloudwatchLoggingOptions): { cloudwatchLoggingOptions+: if std.isArray(v=cloudwatchLoggingOptions) then cloudwatchLoggingOptions else [cloudwatchLoggingOptions] },
        '#withCompressionFormat':: d.fn(help='"The compression format. If no value is specified, the default is UNCOMPRESSED. Other supported values are GZIP, ZIP, Snappy, & HADOOP_SNAPPY."', args=[d.arg(name='compressionFormat', type=d.T.string)]),
        withCompressionFormat(compressionFormat): { compressionFormat: compressionFormat },
        '#withErrorOutputPrefix':: d.fn(help='"Prefix added to failed records before writing them to S3. Not currently supported for redshift destination. This prefix appears immediately following the bucket name. For information about how to specify this prefix, see Custom Prefixes for Amazon S3 Objects."', args=[d.arg(name='errorOutputPrefix', type=d.T.string)]),
        withErrorOutputPrefix(errorOutputPrefix): { errorOutputPrefix: errorOutputPrefix },
        '#withKmsKeyArn':: d.fn(help='"Specifies the KMS key ARN the stream will use to encrypt data. If not set, no encryption will be used."', args=[d.arg(name='kmsKeyArn', type=d.T.string)]),
        withKmsKeyArn(kmsKeyArn): { kmsKeyArn: kmsKeyArn },
        '#withPrefix':: d.fn(help='"The \\"YYYY/MM/DD/HH\\" time format prefix is automatically used for delivered S3 files. You can specify an extra prefix to be added in front of the time format prefix. Note that if the prefix ends with a slash, it appears as a folder in the S3 bucket"', args=[d.arg(name='prefix', type=d.T.string)]),
        withPrefix(prefix): { prefix: prefix },
        '#withRoleArn':: d.fn(help='"The ARN of the AWS credentials."', args=[d.arg(name='roleArn', type=d.T.string)]),
        withRoleArn(roleArn): { roleArn: roleArn },
      },
      '#serverSideEncryption':: d.obj(help='"Encrypt at rest options. Server-side encryption should not be enabled when a kinesis stream is configured as the source of the firehose delivery stream."'),
      serverSideEncryption: {
        '#withEnabled':: d.fn(help='"Enables or disables the logging. Defaults to false."', args=[d.arg(name='enabled', type=d.T.boolean)]),
        withEnabled(enabled): { enabled: enabled },
        '#withKeyArn':: d.fn(help='"Amazon Resource Name (ARN) of the encryption key. Required when key_type is CUSTOMER_MANAGED_CMK."', args=[d.arg(name='keyArn', type=d.T.string)]),
        withKeyArn(keyArn): { keyArn: keyArn },
        '#withKeyType':: d.fn(help='"Type of encryption key. Default is AWS_OWNED_CMK. Valid values are AWS_OWNED_CMK and CUSTOMER_MANAGED_CMK"', args=[d.arg(name='keyType', type=d.T.string)]),
        withKeyType(keyType): { keyType: keyType },
      },
      '#splunkConfiguration':: d.obj(help='"Configuration options if splunk is the destination. More details are given below."'),
      splunkConfiguration: {
        '#cloudwatchLoggingOptions':: d.obj(help='"The CloudWatch Logging Options for the delivery stream. More details are given below"'),
        cloudwatchLoggingOptions: {
          '#withEnabled':: d.fn(help='"Enables or disables the logging. Defaults to false."', args=[d.arg(name='enabled', type=d.T.boolean)]),
          withEnabled(enabled): { enabled: enabled },
          '#withLogGroupName':: d.fn(help='"The CloudWatch group name for logging. This value is required if enabled is true."', args=[d.arg(name='logGroupName', type=d.T.string)]),
          withLogGroupName(logGroupName): { logGroupName: logGroupName },
          '#withLogStreamName':: d.fn(help='"The CloudWatch log stream name for logging. This value is required if enabled is true."', args=[d.arg(name='logStreamName', type=d.T.string)]),
          withLogStreamName(logStreamName): { logStreamName: logStreamName },
        },
        '#processingConfiguration':: d.obj(help='"The data processing configuration.  More details are given below."'),
        processingConfiguration: {
          '#processors':: d.obj(help='"Array of data processors. More details are given below"'),
          processors: {
            '#parameters':: d.obj(help='"Array of processor parameters. More details are given below"'),
            parameters: {
              '#withParameterName':: d.fn(help='"Parameter name. Valid Values: LambdaArn, NumberOfRetries, MetadataExtractionQuery, JsonParsingEngine, RoleArn, BufferSizeInMBs, BufferIntervalInSeconds, SubRecordType, Delimiter. Validation is done against AWS SDK constants; so that values not explicitly listed may also work."', args=[d.arg(name='parameterName', type=d.T.string)]),
              withParameterName(parameterName): { parameterName: parameterName },
              '#withParameterValue':: d.fn(help='"Parameter value. Must be between 1 and 512 length (inclusive). When providing a Lambda ARN, you should specify the resource version as well."', args=[d.arg(name='parameterValue', type=d.T.string)]),
              withParameterValue(parameterValue): { parameterValue: parameterValue },
            },
            '#withParameters':: d.fn(help='"Array of processor parameters. More details are given below"', args=[d.arg(name='parameters', type=d.T.array)]),
            withParameters(parameters): { parameters: if std.isArray(v=parameters) then parameters else [parameters] },
            '#withParametersMixin':: d.fn(help='"Array of processor parameters. More details are given below"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='parameters', type=d.T.array)]),
            withParametersMixin(parameters): { parameters+: if std.isArray(v=parameters) then parameters else [parameters] },
            '#withType':: d.fn(help='"The type of processor. Valid Values: RecordDeAggregation, Lambda, MetadataExtraction, AppendDelimiterToRecord. Validation is done against AWS SDK constants; so that values not explicitly listed may also work."', args=[d.arg(name='type', type=d.T.string)]),
            withType(type): { type: type },
          },
          '#withEnabled':: d.fn(help='"Enables or disables the logging. Defaults to false."', args=[d.arg(name='enabled', type=d.T.boolean)]),
          withEnabled(enabled): { enabled: enabled },
          '#withProcessors':: d.fn(help='"Array of data processors. More details are given below"', args=[d.arg(name='processors', type=d.T.array)]),
          withProcessors(processors): { processors: if std.isArray(v=processors) then processors else [processors] },
          '#withProcessorsMixin':: d.fn(help='"Array of data processors. More details are given below"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='processors', type=d.T.array)]),
          withProcessorsMixin(processors): { processors+: if std.isArray(v=processors) then processors else [processors] },
        },
        '#withCloudwatchLoggingOptions':: d.fn(help='"The CloudWatch Logging Options for the delivery stream. More details are given below"', args=[d.arg(name='cloudwatchLoggingOptions', type=d.T.array)]),
        withCloudwatchLoggingOptions(cloudwatchLoggingOptions): { cloudwatchLoggingOptions: if std.isArray(v=cloudwatchLoggingOptions) then cloudwatchLoggingOptions else [cloudwatchLoggingOptions] },
        '#withCloudwatchLoggingOptionsMixin':: d.fn(help='"The CloudWatch Logging Options for the delivery stream. More details are given below"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='cloudwatchLoggingOptions', type=d.T.array)]),
        withCloudwatchLoggingOptionsMixin(cloudwatchLoggingOptions): { cloudwatchLoggingOptions+: if std.isArray(v=cloudwatchLoggingOptions) then cloudwatchLoggingOptions else [cloudwatchLoggingOptions] },
        '#withHecAcknowledgmentTimeout':: d.fn(help='"The amount of time, in seconds between 180 and 600, that Kinesis Firehose waits to receive an acknowledgment from Splunk after it sends it data."', args=[d.arg(name='hecAcknowledgmentTimeout', type=d.T.number)]),
        withHecAcknowledgmentTimeout(hecAcknowledgmentTimeout): { hecAcknowledgmentTimeout: hecAcknowledgmentTimeout },
        '#withHecEndpoint':: d.fn(help='"The HTTP Event Collector (HEC) endpoint to which Kinesis Firehose sends your data."', args=[d.arg(name='hecEndpoint', type=d.T.string)]),
        withHecEndpoint(hecEndpoint): { hecEndpoint: hecEndpoint },
        '#withHecEndpointType':: d.fn(help='"The HEC endpoint type. Valid values are Raw or Event. The default value is Raw."', args=[d.arg(name='hecEndpointType', type=d.T.string)]),
        withHecEndpointType(hecEndpointType): { hecEndpointType: hecEndpointType },
        '#withHecToken':: d.fn(help='"The GUID that you obtain from your Splunk cluster when you create a new HEC endpoint."', args=[d.arg(name='hecToken', type=d.T.string)]),
        withHecToken(hecToken): { hecToken: hecToken },
        '#withProcessingConfiguration':: d.fn(help='"The data processing configuration.  More details are given below."', args=[d.arg(name='processingConfiguration', type=d.T.array)]),
        withProcessingConfiguration(processingConfiguration): { processingConfiguration: if std.isArray(v=processingConfiguration) then processingConfiguration else [processingConfiguration] },
        '#withProcessingConfigurationMixin':: d.fn(help='"The data processing configuration.  More details are given below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='processingConfiguration', type=d.T.array)]),
        withProcessingConfigurationMixin(processingConfiguration): { processingConfiguration+: if std.isArray(v=processingConfiguration) then processingConfiguration else [processingConfiguration] },
        '#withRetryDuration':: d.fn(help='"The length of time during which Firehose retries delivery after a failure, starting from the initial request and including the first attempt. The default value is 3600 seconds (60 minutes). Firehose does not retry if the value of DurationInSeconds is 0 (zero) or if the first delivery attempt takes longer than the current value."', args=[d.arg(name='retryDuration', type=d.T.number)]),
        withRetryDuration(retryDuration): { retryDuration: retryDuration },
        '#withS3BackupMode':: d.fn(help='"The Amazon S3 backup mode.  Valid values are Disabled and Enabled.  Default value is Disabled."', args=[d.arg(name='s3BackupMode', type=d.T.string)]),
        withS3BackupMode(s3BackupMode): { s3BackupMode: s3BackupMode },
      },
      '#withArn':: d.fn(help='"The Amazon Resource Name (ARN) specifying the Stream"', args=[d.arg(name='arn', type=d.T.string)]),
      withArn(arn): { spec+: { forProvider+: { arn: arn } } },
      '#withDestination':: d.fn(help='"–  This is the destination to where the data is delivered. The only options are s3 (Deprecated, use extended_s3 instead), extended_s3, redshift, elasticsearch, splunk, and http_endpoint."', args=[d.arg(name='destination', type=d.T.string)]),
      withDestination(destination): { spec+: { forProvider+: { destination: destination } } },
      '#withDestinationId':: d.fn(help='', args=[d.arg(name='destinationId', type=d.T.string)]),
      withDestinationId(destinationId): { spec+: { forProvider+: { destinationId: destinationId } } },
      '#withElasticsearchConfiguration':: d.fn(help='"Configuration options if elasticsearch is the destination. More details are given below."', args=[d.arg(name='elasticsearchConfiguration', type=d.T.array)]),
      withElasticsearchConfiguration(elasticsearchConfiguration): { spec+: { forProvider+: { elasticsearchConfiguration: if std.isArray(v=elasticsearchConfiguration) then elasticsearchConfiguration else [elasticsearchConfiguration] } } },
      '#withElasticsearchConfigurationMixin':: d.fn(help='"Configuration options if elasticsearch is the destination. More details are given below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='elasticsearchConfiguration', type=d.T.array)]),
      withElasticsearchConfigurationMixin(elasticsearchConfiguration): { spec+: { forProvider+: { elasticsearchConfiguration+: if std.isArray(v=elasticsearchConfiguration) then elasticsearchConfiguration else [elasticsearchConfiguration] } } },
      '#withExtendedS3Configuration':: d.fn(help='"Enhanced configuration options for the s3 destination. More details are given below."', args=[d.arg(name='extendedS3Configuration', type=d.T.array)]),
      withExtendedS3Configuration(extendedS3Configuration): { spec+: { forProvider+: { extendedS3Configuration: if std.isArray(v=extendedS3Configuration) then extendedS3Configuration else [extendedS3Configuration] } } },
      '#withExtendedS3ConfigurationMixin':: d.fn(help='"Enhanced configuration options for the s3 destination. More details are given below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='extendedS3Configuration', type=d.T.array)]),
      withExtendedS3ConfigurationMixin(extendedS3Configuration): { spec+: { forProvider+: { extendedS3Configuration+: if std.isArray(v=extendedS3Configuration) then extendedS3Configuration else [extendedS3Configuration] } } },
      '#withHttpEndpointConfiguration':: d.fn(help='"Configuration options if http_endpoint is the destination. requires the user to also specify a s3_configuration block.  More details are given below."', args=[d.arg(name='httpEndpointConfiguration', type=d.T.array)]),
      withHttpEndpointConfiguration(httpEndpointConfiguration): { spec+: { forProvider+: { httpEndpointConfiguration: if std.isArray(v=httpEndpointConfiguration) then httpEndpointConfiguration else [httpEndpointConfiguration] } } },
      '#withHttpEndpointConfigurationMixin':: d.fn(help='"Configuration options if http_endpoint is the destination. requires the user to also specify a s3_configuration block.  More details are given below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='httpEndpointConfiguration', type=d.T.array)]),
      withHttpEndpointConfigurationMixin(httpEndpointConfiguration): { spec+: { forProvider+: { httpEndpointConfiguration+: if std.isArray(v=httpEndpointConfiguration) then httpEndpointConfiguration else [httpEndpointConfiguration] } } },
      '#withKinesisSourceConfiguration':: d.fn(help='"Allows the ability to specify the kinesis stream that is used as the source of the firehose delivery stream."', args=[d.arg(name='kinesisSourceConfiguration', type=d.T.array)]),
      withKinesisSourceConfiguration(kinesisSourceConfiguration): { spec+: { forProvider+: { kinesisSourceConfiguration: if std.isArray(v=kinesisSourceConfiguration) then kinesisSourceConfiguration else [kinesisSourceConfiguration] } } },
      '#withKinesisSourceConfigurationMixin':: d.fn(help='"Allows the ability to specify the kinesis stream that is used as the source of the firehose delivery stream."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='kinesisSourceConfiguration', type=d.T.array)]),
      withKinesisSourceConfigurationMixin(kinesisSourceConfiguration): { spec+: { forProvider+: { kinesisSourceConfiguration+: if std.isArray(v=kinesisSourceConfiguration) then kinesisSourceConfiguration else [kinesisSourceConfiguration] } } },
      '#withRedshiftConfiguration':: d.fn(help='"Configuration options if redshift is the destination. Using redshift_configuration requires the user to also specify a s3_configuration block. More details are given below."', args=[d.arg(name='redshiftConfiguration', type=d.T.array)]),
      withRedshiftConfiguration(redshiftConfiguration): { spec+: { forProvider+: { redshiftConfiguration: if std.isArray(v=redshiftConfiguration) then redshiftConfiguration else [redshiftConfiguration] } } },
      '#withRedshiftConfigurationMixin':: d.fn(help='"Configuration options if redshift is the destination. Using redshift_configuration requires the user to also specify a s3_configuration block. More details are given below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='redshiftConfiguration', type=d.T.array)]),
      withRedshiftConfigurationMixin(redshiftConfiguration): { spec+: { forProvider+: { redshiftConfiguration+: if std.isArray(v=redshiftConfiguration) then redshiftConfiguration else [redshiftConfiguration] } } },
      '#withRegion':: d.fn(help="\"If you don't specify an AWS Region, the default is the current region. Region is the region you'd like your resource to be created in.\"", args=[d.arg(name='region', type=d.T.string)]),
      withRegion(region): { spec+: { forProvider+: { region: region } } },
      '#withS3Configuration':: d.fn(help='"Required for non-S3 destinations. For S3 destination, use extended_s3_configuration instead. Configuration options for the s3 destination (or the intermediate bucket if the destination is redshift). More details are given below."', args=[d.arg(name='s3Configuration', type=d.T.array)]),
      withS3Configuration(s3Configuration): { spec+: { forProvider+: { s3Configuration: if std.isArray(v=s3Configuration) then s3Configuration else [s3Configuration] } } },
      '#withS3ConfigurationMixin':: d.fn(help='"Required for non-S3 destinations. For S3 destination, use extended_s3_configuration instead. Configuration options for the s3 destination (or the intermediate bucket if the destination is redshift). More details are given below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='s3Configuration', type=d.T.array)]),
      withS3ConfigurationMixin(s3Configuration): { spec+: { forProvider+: { s3Configuration+: if std.isArray(v=s3Configuration) then s3Configuration else [s3Configuration] } } },
      '#withServerSideEncryption':: d.fn(help='"Encrypt at rest options. Server-side encryption should not be enabled when a kinesis stream is configured as the source of the firehose delivery stream."', args=[d.arg(name='serverSideEncryption', type=d.T.array)]),
      withServerSideEncryption(serverSideEncryption): { spec+: { forProvider+: { serverSideEncryption: if std.isArray(v=serverSideEncryption) then serverSideEncryption else [serverSideEncryption] } } },
      '#withServerSideEncryptionMixin':: d.fn(help='"Encrypt at rest options. Server-side encryption should not be enabled when a kinesis stream is configured as the source of the firehose delivery stream."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='serverSideEncryption', type=d.T.array)]),
      withServerSideEncryptionMixin(serverSideEncryption): { spec+: { forProvider+: { serverSideEncryption+: if std.isArray(v=serverSideEncryption) then serverSideEncryption else [serverSideEncryption] } } },
      '#withSplunkConfiguration':: d.fn(help='"Configuration options if splunk is the destination. More details are given below."', args=[d.arg(name='splunkConfiguration', type=d.T.array)]),
      withSplunkConfiguration(splunkConfiguration): { spec+: { forProvider+: { splunkConfiguration: if std.isArray(v=splunkConfiguration) then splunkConfiguration else [splunkConfiguration] } } },
      '#withSplunkConfigurationMixin':: d.fn(help='"Configuration options if splunk is the destination. More details are given below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='splunkConfiguration', type=d.T.array)]),
      withSplunkConfigurationMixin(splunkConfiguration): { spec+: { forProvider+: { splunkConfiguration+: if std.isArray(v=splunkConfiguration) then splunkConfiguration else [splunkConfiguration] } } },
      '#withTags':: d.fn(help='"Key-value map of resource tags."', args=[d.arg(name='tags', type=d.T.object)]),
      withTags(tags): { spec+: { forProvider+: { tags: tags } } },
      '#withTagsMixin':: d.fn(help='"Key-value map of resource tags."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='tags', type=d.T.object)]),
      withTagsMixin(tags): { spec+: { forProvider+: { tags+: tags } } },
      '#withVersionId':: d.fn(help='"Specifies the table version for the output data schema. Defaults to LATEST."', args=[d.arg(name='versionId', type=d.T.string)]),
      withVersionId(versionId): { spec+: { forProvider+: { versionId: versionId } } },
    },
    '#providerConfigRef':: d.obj(help='"ProviderConfigReference specifies how the provider that will be used to create, observe, update, and delete this managed resource should be configured."'),
    providerConfigRef: {
      '#policy':: d.obj(help='"Policies for referencing."'),
      policy: {
        '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
        withResolution(resolution): { spec+: { providerConfigRef+: { policy+: { resolution: resolution } } } },
        '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
        withResolve(resolve): { spec+: { providerConfigRef+: { policy+: { resolve: resolve } } } },
      },
      '#withName':: d.fn(help='"Name of the referenced object."', args=[d.arg(name='name', type=d.T.string)]),
      withName(name): { spec+: { providerConfigRef+: { name: name } } },
    },
    '#providerRef':: d.obj(help='"ProviderReference specifies the provider that will be used to create, observe, update, and delete this managed resource. Deprecated: Please use ProviderConfigReference, i.e. `providerConfigRef`"'),
    providerRef: {
      '#policy':: d.obj(help='"Policies for referencing."'),
      policy: {
        '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
        withResolution(resolution): { spec+: { providerRef+: { policy+: { resolution: resolution } } } },
        '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
        withResolve(resolve): { spec+: { providerRef+: { policy+: { resolve: resolve } } } },
      },
      '#withName':: d.fn(help='"Name of the referenced object."', args=[d.arg(name='name', type=d.T.string)]),
      withName(name): { spec+: { providerRef+: { name: name } } },
    },
    '#publishConnectionDetailsTo':: d.obj(help='"PublishConnectionDetailsTo specifies the connection secret config which contains a name, metadata and a reference to secret store config to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource."'),
    publishConnectionDetailsTo: {
      '#configRef':: d.obj(help='"SecretStoreConfigRef specifies which secret store config should be used for this ConnectionSecret."'),
      configRef: {
        '#policy':: d.obj(help='"Policies for referencing."'),
        policy: {
          '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
          withResolution(resolution): { spec+: { publishConnectionDetailsTo+: { configRef+: { policy+: { resolution: resolution } } } } },
          '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
          withResolve(resolve): { spec+: { publishConnectionDetailsTo+: { configRef+: { policy+: { resolve: resolve } } } } },
        },
        '#withName':: d.fn(help='"Name of the referenced object."', args=[d.arg(name='name', type=d.T.string)]),
        withName(name): { spec+: { publishConnectionDetailsTo+: { configRef+: { name: name } } } },
      },
      '#metadata':: d.obj(help='"Metadata is the metadata for connection secret."'),
      metadata: {
        '#withAnnotations':: d.fn(help='"Annotations are the annotations to be added to connection secret. - For Kubernetes secrets, this will be used as \\"metadata.annotations\\". - It is up to Secret Store implementation for others store types."', args=[d.arg(name='annotations', type=d.T.object)]),
        withAnnotations(annotations): { spec+: { publishConnectionDetailsTo+: { metadata+: { annotations: annotations } } } },
        '#withAnnotationsMixin':: d.fn(help='"Annotations are the annotations to be added to connection secret. - For Kubernetes secrets, this will be used as \\"metadata.annotations\\". - It is up to Secret Store implementation for others store types."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='annotations', type=d.T.object)]),
        withAnnotationsMixin(annotations): { spec+: { publishConnectionDetailsTo+: { metadata+: { annotations+: annotations } } } },
        '#withLabels':: d.fn(help='"Labels are the labels/tags to be added to connection secret. - For Kubernetes secrets, this will be used as \\"metadata.labels\\". - It is up to Secret Store implementation for others store types."', args=[d.arg(name='labels', type=d.T.object)]),
        withLabels(labels): { spec+: { publishConnectionDetailsTo+: { metadata+: { labels: labels } } } },
        '#withLabelsMixin':: d.fn(help='"Labels are the labels/tags to be added to connection secret. - For Kubernetes secrets, this will be used as \\"metadata.labels\\". - It is up to Secret Store implementation for others store types."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='labels', type=d.T.object)]),
        withLabelsMixin(labels): { spec+: { publishConnectionDetailsTo+: { metadata+: { labels+: labels } } } },
        '#withType':: d.fn(help='"Type is the SecretType for the connection secret. - Only valid for Kubernetes Secret Stores."', args=[d.arg(name='type', type=d.T.string)]),
        withType(type): { spec+: { publishConnectionDetailsTo+: { metadata+: { type: type } } } },
      },
      '#withName':: d.fn(help='"Name is the name of the connection secret."', args=[d.arg(name='name', type=d.T.string)]),
      withName(name): { spec+: { publishConnectionDetailsTo+: { name: name } } },
    },
    '#withDeletionPolicy':: d.fn(help='"DeletionPolicy specifies what will happen to the underlying external when this managed resource is deleted - either \\"Delete\\" or \\"Orphan\\" the external resource."', args=[d.arg(name='deletionPolicy', type=d.T.string)]),
    withDeletionPolicy(deletionPolicy): { spec+: { deletionPolicy: deletionPolicy } },
    '#writeConnectionSecretToRef':: d.obj(help='"WriteConnectionSecretToReference specifies the namespace and name of a Secret to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource. This field is planned to be replaced in a future release in favor of PublishConnectionDetailsTo. Currently, both could be set independently and connection details would be published to both without affecting each other."'),
    writeConnectionSecretToRef: {
      '#withName':: d.fn(help='"Name of the secret."', args=[d.arg(name='name', type=d.T.string)]),
      withName(name): { spec+: { writeConnectionSecretToRef+: { name: name } } },
      '#withNamespace':: d.fn(help='"Namespace of the secret."', args=[d.arg(name='namespace', type=d.T.string)]),
      withNamespace(namespace): { spec+: { writeConnectionSecretToRef+: { namespace: namespace } } },
    },
  },
  '#mixin': 'ignore',
  mixin: self,
}
